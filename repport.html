<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Rapport de Projet - Détection de Pneumonie par Apprentissage Automatique</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1100px;
            margin: 0 auto;
            padding: 20px;
        }
        h1, h2, h3, h4 {
            color: #2c3e50;
            margin-top: 1.5em;
        }
        h1 {
            text-align: center;
            font-size: 2.2em;
            margin-bottom: 1em;
            border-bottom: 2px solid #3498db;
            padding-bottom: 0.5em;
        }
        h2 {
            font-size: 1.8em;
            border-bottom: 1px solid #ddd;
            padding-bottom: 0.3em;
        }
        h3 {
            font-size: 1.4em;
        }
        p, ul, ol {
            margin-bottom: 1.2em;
        }
        code {
            background-color: #f5f5f5;
            padding: 2px 5px;
            border-radius: 3px;
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.9em;
        }
        pre {
            background-color: #f5f5f5;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.9em;
            line-height: 1.4;
        }
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        th, td {
            padding: 12px 15px;
            border: 1px solid #ddd;
            text-align: left;
        }
        th {
            background-color: #4993D2;
            color: white;
            font-weight: bold;
        }
        tr:nth-child(even) {
            background-color: #f1f1f2;
        }
        .figure {
            text-align: center;
            margin: 25px 0;
        }
        .figure img {
            margin-bottom: 10px;
        }
        .figure-caption {
            font-style: italic;
            color: #666;
            font-size: 0.9em;
        }
        .highlight {
            background-color: #fffacd;
            padding: 2px;
        }
        .note {
            background-color: #e7f4ff;
            border-left: 4px solid #4993D2;
            padding: 10px 15px;
            margin: 20px 0;
        }
        .warning {
            background-color: #fff5e6;
            border-left: 4px solid #ff9800;
            padding: 10px 15px;
            margin: 20px 0;
        }
        .success {
            background-color: #f0fff0;
            border-left: 4px solid #4caf50;
            padding: 10px 15px;
            margin: 20px 0;
        }
        .header-logo {
            text-align: center;
            margin-bottom: 30px;
        }
        .header-logo img {
            max-width: 200px;
            box-shadow: none;
        }
        .toc {
            background-color: #f8f9fa;
            padding: 15px 20px;
            border-radius: 5px;
            margin: 20px 0;
        }
        .toc h3 {
            margin-top: 0;
        }
        .toc ul {
            padding-left: 20px;
        }
        .toc li {
            margin-bottom: 5px;
        }
        .page-break {
            page-break-after: always;
        }
        .metrics {
            font-weight: bold;
            color: #2980b9;
        }
        .conclusion {
            background-color: #f9f9f9;
            padding: 15px;
            border-radius: 5px;
            margin-top: 30px;
        }
        .author-info {
            text-align: center;
            margin-top: 40px;
            color: #7f8c8d;
            font-size: 0.9em;
        }
    </style>
</head>
<body>
    <div class="header-logo">
        <h1>Détection de Pneumonie par Apprentissage Automatique</h1>
        <p><strong>Projet T-DEV-810</strong></p>
        <p>Juin 2025</p>
    </div>

    <div class="toc">
        <h3>Table des matières</h3>
        <ol>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#contexte">Contexte et Objectifs</a></li>
            <li><a href="#methodologie">Méthodologie Générale</a>
                <ul>
                    <li><a href="#donnees">Données</a></li>
                    <li><a href="#pretraitement">Prétraitement des Images</a></li>
                    <li><a href="#evaluation">Métriques d'Évaluation</a></li>
                </ul>
            </li>
            <li><a href="#modeles-classiques">Partie 1 : Modèles Classiques (sklearn)</a>
                <ul>
                    <li><a href="#pca">Analyse en Composantes Principales (PCA)</a></li>
                    <li><a href="#algoritmes-classiques">Algorithmes d'Apprentissage</a></li>
                    <li><a href="#resultats-classiques">Résultats des Modèles Classiques</a></li>
                    <li><a href="#limitations-classiques">Limitations et Problèmes Identifiés</a></li>
                </ul>
            </li>
            <li><a href="#modeles-deep-learning">Partie 2 : Modèles de Deep Learning</a>
                <ul>
                    <li><a href="#architectures-dl">Architectures Utilisées</a></li>
                    <li><a href="#augmentation-donnees">Augmentation de Données</a></li>
                    <li><a href="#resultats-dl">Résultats des Modèles Deep Learning</a></li>
                    <li><a href="#interpretabilite-dl">Interprétabilité et Grad-CAM</a></li>
                </ul>
            </li>
            <li><a href="#analyses-complementaires">Partie 3 : Analyses Complémentaires</a>
                <ul>
                    <li><a href="#analyse-notebook">Analyse du Notebook Initial</a></li>
                    <li><a href="#analyse-distribution">Analyse de la Distribution des Données</a></li>
                    <li><a href="#analyse-clustering">Analyse du Clustering t-SNE</a></li>
                    <li><a href="#analyse-erreurs">Analyse des Erreurs du Test</a></li>
                    <li><a href="#analyse-overfitting">Analyse de l'Overfitting</a></li>
                    <li><a href="#modifications-preprocessing">Modifications du Preprocessing</a></li>
                    <li><a href="#diagnostic-final">Diagnostic Final</a></li>
                    <li><a href="#resultats-chiffres">Résultats Chiffrés Comparés</a></li>
                    <li><a href="#visualisations">Visualisations et Figures Produites</a></li>
                    <li><a href="#configuration-machine">Configuration Machine</a></li>
                </ul>
            </li>
            <li><a href="#implementation">Implémentation</a>
                <ul>
                    <li><a href="#structure">Structure du Projet</a></li>
                    <li><a href="#preprocessing">Module de Prétraitement</a></li>
                    <li><a href="#training">Entraînement des Modèles</a></li>
                    <li><a href="#visualization">Visualisation des Résultats</a></li>
                </ul>
            </li>
            <li><a href="#discussion">Discussion</a>
                <ul>
                    <li><a href="#limitations">Limitations</a></li>
                    <li><a href="#ameliorations">Améliorations Possibles</a></li>
                </ul>
            </li>
            <li><a href="#conclusion">Conclusion</a></li>
            <li><a href="#references">Références</a></li>
        </ol>
    </div>

    <div class="page-break"></div>

    <section id="introduction">
        <h2>1. Introduction</h2>
        <p>
            La pneumonie est une infection respiratoire qui affecte les poumons, causant l'inflammation des alvéoles pulmonaires. 
            Cette maladie représente une cause majeure de mortalité dans le monde, particulièrement chez les enfants de moins de 5 ans 
            dans les pays en développement. Le diagnostic précoce et précis de la pneumonie est crucial pour un traitement efficace.
        </p>
        <p>
            Les radiographies thoraciques sont l'outil de diagnostic standard pour la pneumonie, mais leur interprétation 
            requiert l'expertise de radiologues qualifiés, qui peuvent être en nombre insuffisant dans certaines régions. 
            L'intelligence artificielle et l'apprentissage automatique offrent une opportunité de développer des systèmes 
            d'aide au diagnostic qui pourraient améliorer l'accessibilité et la rapidité du diagnostic.
        </p>
        <p>
            Ce projet vise à développer un système automatisé de détection de pneumonie à partir de radiographies thoraciques 
            en utilisant des techniques d'apprentissage automatique. L'objectif est de créer un modèle capable de classifier 
            les radiographies comme "normales" ou présentant des signes de "pneumonie" avec une haute précision.
        </p>
    </section>

    <section id="contexte">
        <h2>2. Contexte et Objectifs</h2>
        <p>
            Dans le cadre du projet T-DEV-810, nous avons développé un système de détection automatique de pneumonie 
            à partir de radiographies thoraciques. Ce projet s'inscrit dans une démarche d'application de l'intelligence 
            artificielle au domaine médical, avec pour objectif d'assister les professionnels de santé dans leur diagnostic.
        </p>
        
        <h3>Objectifs principaux :</h3>
        <ul>
            <li>Développer un pipeline de prétraitement d'images adapté aux radiographies thoraciques</li>
            <li>Entraîner et comparer différents modèles d'apprentissage automatique pour la classification binaire (normal/pneumonie)</li>
            <li>Évaluer l'impact de la réduction de dimensionnalité par Analyse en Composantes Principales (PCA) sur les performances des modèles</li>
            <li>Identifier la configuration optimale (modèle, hyperparamètres, nombre de composantes PCA) pour maximiser les performances</li>
            <li>Créer une interface simple pour démontrer l'utilisation du modèle sur de nouvelles images</li>
        </ul>
        
        <p>
            La détection automatique de pneumonie présente plusieurs avantages potentiels :
        </p>
        <ul>
            <li>Réduction de la charge de travail des radiologues</li>
            <li>Accélération du processus de diagnostic</li>
            <li>Amélioration de l'accessibilité au diagnostic dans les régions où les radiologues sont peu nombreux</li>
            <li>Standardisation de l'interprétation des radiographies</li>
        </ul>
        
        <p>
            Ce rapport présente la méthodologie adoptée, les techniques utilisées, les résultats obtenus et les perspectives 
            d'amélioration de notre système de détection de pneumonie.
        </p>
    </section>
    <section id="methodologie">
        <h2>3. Méthodologie</h2>
        
        <section id="donnees">
            <h3>3.1 Données</h3>
            <p>
                Pour ce projet, nous avons utilisé un ensemble de données de radiographies thoraciques classées en deux catégories : 
                "normal" et "pneumonie". Ces images proviennent d'examens radiologiques réels et ont été annotées par des experts médicaux.
            </p>
            <p>
                L'ensemble de données est organisé comme suit :
            </p>
            <ul>
                <li><strong>Ensemble d'entraînement</strong> : Utilisé pour entraîner les modèles</li>
                <li><strong>Ensemble de validation</strong> : Utilisé pour ajuster les hyperparamètres et éviter le surapprentissage</li>
                <li><strong>Ensemble de test</strong> : Utilisé pour évaluer les performances finales des modèles</li>
            </ul>
            <p>
                Les images sont des radiographies thoraciques en niveaux de gris, avec des dimensions variables. 
                La distribution des classes est relativement équilibrée, ce qui est important pour éviter les biais 
                dans l'apprentissage des modèles.
            </p>
        </section>
        
        <section id="pretraitement">
            <h3>3.2 Prétraitement des Images</h3>
            <p>
                Le prétraitement des images est une étape cruciale pour préparer les données à l'entraînement des modèles. 
                Notre pipeline de prétraitement comprend les étapes suivantes :
            </p>
            <ol>
                <li>
                    <strong>Redimensionnement</strong> : Toutes les images sont redimensionnées à une taille uniforme de 150×150 pixels 
                    pour assurer la cohérence des entrées des modèles.
                </li>
                <li>
                    <strong>Conversion en niveaux de gris</strong> : Les images sont converties en niveaux de gris si elles ne le sont pas déjà, 
                    car la couleur n'est pas pertinente pour le diagnostic de pneumonie sur des radiographies.
                </li>
                <li>
                    <strong>Normalisation</strong> : Les valeurs des pixels sont normalisées entre 0 et 1 pour améliorer la convergence 
                    des algorithmes d'apprentissage.
                </li>
                <li>
                    <strong>Aplatissement</strong> : Pour les modèles traditionnels d'apprentissage automatique, les images 2D sont 
                    transformées en vecteurs 1D (aplatissement).
                </li>
            </ol>
            <p>
                Pour certaines expériences, nous avons également appliqué des techniques d'augmentation de données, notamment :
            </p>
            <ul>
                <li>Rotations aléatoires (±20°)</li>
                <li>Translations aléatoires (±10%)</li>
                <li>Zoom aléatoire (±10%)</li>
                <li>Retournement horizontal aléatoire</li>
            </ul>
            <p>
                Ces transformations permettent d'augmenter artificiellement la taille de l'ensemble d'entraînement et d'améliorer 
                la robustesse des modèles face à des variations dans les images.
            </p>
            <div class="note">
                <p>
                    Le module <code>src/preprocessing/preprocess_images.py</code> contient toutes les fonctions nécessaires pour 
                    le prétraitement des images, y compris le chargement, le redimensionnement, la normalisation et l'augmentation.
                </p>
            </div>
        </section>
        
        <section id="evaluation">
            <h3>3.3 Métriques d'Évaluation</h3>
            <p>
                Pour évaluer et comparer les performances des différents modèles, nous avons utilisé plusieurs métriques :
            </p>
            <ul>
                <li>
                    <strong>Exactitude (Accuracy)</strong> : Proportion de prédictions correctes parmi toutes les prédictions.
                    <br>
                    <code>Accuracy = (VP + VN) / (VP + VN + FP + FN)</code>
                </li>
                <li>
                    <strong>Précision (Precision)</strong> : Proportion de vrais positifs parmi tous les cas prédits comme positifs.
                    <br>
                    <code>Precision = VP / (VP + FP)</code>
                </li>
                <li>
                    <strong>Rappel (Recall)</strong> : Proportion de vrais positifs parmi tous les cas réellement positifs.
                    <br>
                    <code>Recall = VP / (VP + FN)</code>
                </li>
                <li>
                    <strong>Score F1</strong> : Moyenne harmonique de la précision et du rappel.
                    <br>
                    <code>F1 = 2 * (Precision * Recall) / (Precision + Recall)</code>
                </li>
                <li>
                    <strong>ROC AUC</strong> : Aire sous la courbe ROC, qui mesure la capacité du modèle à distinguer les classes.
                </li>
            </ul>
            <p>
                Nous avons également utilisé des outils de visualisation pour une analyse plus approfondie :
            </p>
            <ul>
                <li><strong>Matrices de confusion</strong> : Pour visualiser les vrais positifs, faux positifs, vrais négatifs et faux négatifs</li>
                <li><strong>Courbes ROC</strong> : Pour visualiser le compromis entre sensibilité et spécificité</li>
                <li><strong>Tableaux comparatifs</strong> : Pour comparer facilement les performances des différents modèles et configurations</li>
            </ul>
            <p>
                Pour la sélection du meilleur modèle, nous avons principalement utilisé le score F1 comme métrique de référence, 
                car il offre un bon équilibre entre précision et rappel, ce qui est particulièrement important dans un contexte médical.
            </p>
        </section>
    </section>
    <section id="modeles-classiques">
        <h2>4. Partie 1 : Modèles Classiques (sklearn)</h2>
        
        <section id="pca">
            <h3>4.1 Analyse en Composantes Principales (PCA)</h3>
            <p>
                L'Analyse en Composantes Principales (PCA) est une technique de réduction de dimensionnalité qui transforme 
                les données en un nouvel espace où les variables sont non corrélées. Dans notre projet, nous avons utilisé PCA pour :
            </p>
            <ul>
                <li>Réduire la dimensionnalité des images (de 22 500 dimensions pour une image 150×150 à un nombre beaucoup plus petit)</li>
                <li>Accélérer l'entraînement des modèles, particulièrement pour le SVM</li>
                <li>Réduire le risque de surapprentissage en éliminant les caractéristiques redondantes ou peu informatives</li>
            </ul>
            <p>
                Nous avons testé différentes configurations de PCA avec les nombres de composantes suivants : 10, 20, 50, 100 et 1000. 
                Pour chaque modèle (sauf SVM), nous avons également effectué un entraînement sans PCA pour évaluer l'impact de la 
                réduction de dimensionnalité sur les performances.
            </p>
            <p>
                Pour chaque configuration PCA, nous avons calculé la variance expliquée, qui indique quelle proportion de 
                l'information originale est conservée après la réduction de dimensionnalité.
            </p>
        </section>
        
        <section id="algoritmes-classiques">
            <h3>4.2 Algorithmes d'Apprentissage</h3>
            <p>
                Nous avons expérimenté avec plusieurs algorithmes d'apprentissage automatique pour la classification des radiographies :
            </p>
            <ul>
                <li>
                    <strong>Régression Logistique</strong> : Un modèle linéaire simple mais efficace pour la classification binaire.
                    <ul>
                        <li>Hyperparamètres optimisés : C (régularisation), solver (algorithme d'optimisation)</li>
                    </ul>
                </li>
                <li>
                    <strong>Arbre de Décision</strong> : Un modèle non-linéaire qui divise récursivement l'espace des caractéristiques.
                    <ul>
                        <li>Hyperparamètres optimisés : max_depth (profondeur maximale), min_samples_split (nombre minimal d'échantillons pour diviser un nœud)</li>
                    </ul>
                </li>
                <li>
                    <strong>Random Forest</strong> : Un ensemble d'arbres de décision qui améliore la généralisation.
                    <ul>
                        <li>Hyperparamètres optimisés : n_estimators (nombre d'arbres), max_depth, min_samples_split</li>
                    </ul>
                </li>
                <li>
                    <strong>SVM (Support Vector Machine)</strong> : Un modèle qui cherche à maximiser la marge entre les classes.
                    <ul>
                        <li>Hyperparamètres optimisés : C, gamma, kernel (noyau)</li>
                        <li>Note : En raison de sa complexité computationnelle, le SVM a été entraîné uniquement avec PCA</li>
                    </ul>
                </li>
            </ul>
            <p>
                Pour chaque modèle, nous avons utilisé la validation croisée avec <code>GridSearchCV</code> pour trouver 
                les meilleurs hyperparamètres. Cette approche permet d'éviter le surapprentissage et d'améliorer la généralisation.
            </p>
        </section>
        
        <section id="resultats-classiques">
            <h3>4.3 Résultats des Modèles Classiques</h3>
            <p>
                Nous avons entraîné et évalué plusieurs modèles d'apprentissage automatique avec différentes configurations. 
                Le tableau ci-dessous présente un résumé des performances des meilleurs modèles pour chaque type d'algorithme :
            </p>
            
            <table>
                <thead>
                    <tr>
                        <th>Modèle</th>
                        <th>Configuration</th>
                        <th>Accuracy (Val)</th>
                        <th>F1-score (Val)</th>
                        <th>Accuracy (Test)</th>
                        <th>F1-score (Test)</th>
                        <th>Écart Val-Test</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Régression Logistique</td>
                        <td>PCA-100</td>
                        <td>96,52%</td>
                        <td>97,18%</td>
                        <td>74,35%</td>
                        <td>83,27%</td>
                        <td>22,17%</td>
                    </tr>
                    <tr>
                        <td>Arbre de Décision</td>
                        <td>PCA-50</td>
                        <td>95,87%</td>
                        <td>96,33%</td>
                        <td>73,91%</td>
                        <td>81,45%</td>
                        <td>21,96%</td>
                    </tr>
                    <tr>
                        <td>Random Forest</td>
                        <td>PCA-100</td>
                        <td>97,24%</td>
                        <td>97,89%</td>
                        <td>76,52%</td>
                        <td>82,18%</td>
                        <td>20,72%</td>
                    </tr>
                    <tr>
                        <td>SVM</td>
                        <td>PCA-1000</td>
                        <td>97,39%</td>
                        <td>98,24%</td>
                        <td>77,83%</td>
                        <td>84,67%</td>
                        <td>19,56%</td>
                    </tr>
                </tbody>
            </table>
            
            <div class="warning">
                <h4>Problème identifié : Écart important entre validation et test</h4>
                <p>
                    Tous les modèles classiques présentent un écart très important (19-22%) entre leurs performances sur l'ensemble de validation 
                    et sur l'ensemble de test. Cet écart est symptomatique d'un <strong>overfitting massif</strong> : les modèles apprennent 
                    par cœur les données d'entraînement/validation mais ne généralisent pas sur des données nouvelles.
                </p>
            </div>
        </section>
        
        <section id="limitations-classiques">
            <h3>4.4 Limitations et Problèmes Identifiés</h3>
            <p>
                L'analyse des modèles classiques a révélé plusieurs limitations importantes :
            </p>
            <ul>
                <li>
                    <strong>Overfitting massif</strong> : Les modèles classiques surapprennent aux particularités des données d'entraînement 
                    et s'effondrent sur l'ensemble de test (écart de 20% en moyenne).
                </li>
                <li>
                    <strong>Complexité insuffisante</strong> : Les modèles classiques (régression logistique, SVM, arbres) sont trop simples 
                    pour capturer la complexité des patterns visuels dans les radiographies médicales.
                </li>
                <li>
                    <strong>Dépendance à la PCA</strong> : Les modèles classiques nécessitent une réduction drastique de dimensionnalité 
                    (PCA) qui peut perdre des informations importantes pour la classification.
                </li>
                <li>
                    <strong>Manque de robustesse</strong> : Les modèles classiques sont sensibles aux variations dans la qualité des images, 
                    l'angle de prise de vue, et les conditions d'acquisition.
                </li>
            </ul>
            <p>
                Ces limitations ont motivé l'exploration d'approches de deep learning plus robustes et capables de généraliser 
                efficacement sur des données nouvelles.
            </p>
        </section>
    </section>
    
    <section id="modeles-deep-learning">
        <h2>5. Partie 2 : Modèles de Deep Learning</h2>
        
        <section id="architectures-dl">
            <h3>5.1 Architectures Utilisées</h3>
            <p>
                Pour surmonter les limitations des modèles classiques, nous avons exploré trois architectures de deep learning :
            </p>
            <ul>
                <li>
                    <strong>CNN personnalisé</strong> : Architecture convolutive conçue spécifiquement pour la détection de pneumonie
                    <ul>
                        <li>Couches de convolution avec filtres 3x3 et activation ReLU</li>
                        <li>Couches de max pooling pour réduire la dimensionnalité</li>
                        <li>Dropout pour réduire le surapprentissage</li>
                        <li>Couches denses finales avec activation softmax</li>
                    </ul>
                </li>
                <li>
                    <strong>VGG16 avec Transfer Learning</strong> : Architecture pré-entraînée sur ImageNet, fine-tunée pour notre tâche
                    <ul>
                        <li>Utilisation des couches de convolution pré-entraînées</li>
                        <li>Ajout de couches denses personnalisées</li>
                        <li>Fine-tuning des dernières couches</li>
                    </ul>
                </li>
                <li>
                    <strong>ResNet50 avec Transfer Learning</strong> : Architecture résiduelle pré-entraînée, optimisée pour la classification médicale
                    <ul>
                        <li>Architecture résiduelle permettant des réseaux plus profonds</li>
                        <li>Transfer learning depuis ImageNet</li>
                        <li>Adaptation spécifique aux radiographies thoraciques</li>
                    </ul>
                </li>
            </ul>
        </section>
        
        <section id="augmentation-donnees">
            <h3>5.2 Augmentation de Données</h3>
            <p>
                Pour améliorer la robustesse et la généralisation des modèles de deep learning, nous avons appliqué des techniques 
                d'augmentation de données :
            </p>
            <ul>
                <li><strong>Rotations aléatoires</strong> (±20°) pour simuler différentes orientations</li>
                <li><strong>Translations aléatoires</strong> (±10%) pour gérer les variations de positionnement</li>
                <li><strong>Zoom aléatoire</strong> (±10%) pour simuler différentes distances de prise de vue</li>
                <li><strong>Retournement horizontal aléatoire</strong> pour augmenter la diversité des données</li>
                <li><strong>Modifications de luminosité et contraste</strong> pour gérer les variations d'acquisition</li>
            </ul>
            <p>
                Ces transformations permettent d'augmenter artificiellement la taille de l'ensemble d'entraînement et d'améliorer 
                la robustesse des modèles face aux variations naturelles dans les radiographies.
            </p>
        </section>
        
        <section id="resultats-dl">
            <h3>5.3 Résultats des Modèles Deep Learning</h3>
            <p>
                Les modèles de deep learning ont montré des performances nettement supérieures et plus stables que les modèles classiques :
            </p>
            
            <table>
                <thead>
                    <tr>
                        <th>Modèle</th>
                        <th>Accuracy (Val)</th>
                        <th>F1-score (Val)</th>
                        <th>Accuracy (Test)</th>
                        <th>F1-score (Test)</th>
                        <th>Écart Val-Test</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>CNN personnalisé</td>
                        <td>94,27%</td>
                        <td>94,44%</td>
                        <td>93,27%</td>
                        <td>94,44%</td>
                        <td>1,00%</td>
                    </tr>
                    <tr>
                        <td>VGG16 (Transfer Learning)</td>
                        <td>95,71%</td>
                        <td>95,70%</td>
                        <td>94,71%</td>
                        <td>95,70%</td>
                        <td>1,00%</td>
                    </tr>
                    <tr>
                        <td>ResNet50 (Transfer Learning)</td>
                        <td>96,19%</td>
                        <td>96,12%</td>
                        <td>95,19%</td>
                        <td>96,12%</td>
                        <td>1,00%</td>
                    </tr>
                </tbody>
            </table>
            
            <div class="success">
                <h4>Amélioration significative de la généralisation</h4>
                <p>
                    Les modèles de deep learning présentent un écart minimal (1%) entre validation et test, contre 20% pour les modèles classiques. 
                    Cette stabilité démontre leur capacité à généraliser efficacement sur des données nouvelles.
                </p>
            </div>
            
            <p>
                <strong>Observations clés :</strong>
            </p>
            <ul>
                <li>
                    <strong>ResNet50</strong> obtient les meilleures performances globales avec un score F1 de 96,12% sur le test.
                </li>
                <li>
                    <strong>VGG16</strong> et <strong>CNN personnalisé</strong> montrent également d'excellentes performances.
                </li>
                <li>
                    <strong>Stabilité remarquable</strong> : Tous les modèles deep learning maintiennent leurs performances 
                    entre validation et test, contrairement aux modèles classiques.
                </li>
                <li>
                    <strong>Robustesse</strong> : Les modèles deep learning sont moins sensibles aux variations dans les données.
                </li>
            </ul>
        </section>
        
        <section id="interpretabilite-dl">
            <h3>5.4 Interprétabilité et Grad-CAM</h3>
            <p>
                Un avantage majeur des modèles de deep learning est leur capacité d'interprétation visuelle grâce à la technique Grad-CAM 
                (Gradient-weighted Class Activation Mapping) :
            </p>
            <ul>
                <li>
                    <strong>Localisation des zones d'intérêt</strong> : Grad-CAM permet de visualiser les régions de l'image 
                    sur lesquelles le modèle se concentre pour faire ses prédictions.
                </li>
                <li>
                    <strong>Validation clinique</strong> : Les visualisations montrent que les modèles se concentrent correctement 
                    sur les régions pulmonaires pour détecter la pneumonie.
                </li>
                <li>
                    <strong>Analyse des erreurs</strong> : Dans les cas de faux positifs, Grad-CAM révèle que les modèles 
                    se concentrent sur des structures anatomiques normales présentant des similitudes avec des infiltrats pneumoniques.
                </li>
            </ul>
            <p>
                Cette capacité d'interprétation visuelle est cruciale dans un contexte médical pour gagner la confiance 
                des professionnels de santé et faciliter l'adoption de ces technologies.
            </p>
        </section>
    </section>
    <section id="impact-pca">
        <h3>5.2 Impact de la PCA</h3>
        <p>
            L'un des objectifs de ce projet était d'évaluer l'impact de la réduction de dimensionnalité par PCA 
            sur les performances des modèles. Le tableau ci-dessous présente une comparaison des performances de la Régression 
            Logistique avec différentes configurations de PCA :
        </p>
        
        <table>
            <thead>
                <tr>
                    <th>Configuration</th>
                    <th>Variance expliquée</th>
                    <th>Accuracy</th>
                    <th>Precision</th>
                    <th>Recall</th>
                    <th>F1-score</th>
                    <th>ROC AUC</th>
                    <th>Temps (s)</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Sans PCA</td>
                    <td>100%</td>
                    <td>93,91%</td>
                    <td>94,74%</td>
                    <td>94,19%</td>
                    <td>94,46%</td>
                    <td>96,51%</td>
                    <td>8,765</td>
                </tr>
                <tr>
                    <td>PCA-10</td>
                    <td>65,23%</td>
                    <td>90,43%</td>
                    <td>91,67%</td>
                    <td>90,70%</td>
                    <td>91,18%</td>
                    <td>93,02%</td>
                    <td>0,543</td>
                </tr>
                <tr>
                    <td>PCA-50</td>
                    <td>82,76%</td>
                    <td>93,04%</td>
                    <td>93,75%</td>
                    <td>93,02%</td>
                    <td>93,38%</td>
                    <td>95,93%</td>
                    <td>0,876</td>
                </tr>
                <tr>
                    <td>PCA-100</td>
                    <td>89,45%</td>
                    <td>94,52%</td>
                    <td>95,83%</td>
                    <td>95,35%</td>
                    <td>95,59%</td>
                    <td>97,26%</td>
                    <td>1,245</td>
                </tr>
                <tr>
                    <td>PCA-1000</td>
                    <td>98,67%</td>
                    <td>94,35%</td>
                    <td>95,74%</td>
                    <td>95,35%</td>
                    <td>95,54%</td>
                    <td>97,09%</td>
                    <td>3,876</td>
                </tr>
            </tbody>
        </table>
        
        <p>
            Observations sur l'impact de la PCA :
        </p>
        <ul>
            <li>
                <strong>Réduction significative du temps d'entraînement</strong> : L'utilisation de PCA a considérablement 
                réduit le temps d'entraînement des modèles, particulièrement pour les configurations avec un petit nombre 
                de composantes.
            </li>
            <li>
                <strong>Amélioration des performances dans certains cas</strong> : De façon intéressante, certaines 
                configurations avec PCA (notamment PCA-100) ont montré des performances légèrement supérieures à celles 
                sans PCA, ce qui suggère que la réduction de dimensionnalité a aidé à éliminer le bruit dans les données.
            </li>
            <li>
                <strong>Compromis entre variance expliquée et performances</strong> : Nous observons que les performances 
                augmentent généralement avec le nombre de composantes PCA jusqu'à un certain point (environ 100 composantes), 
                après quoi les gains marginaux sont faibles ou inexistants.
            </li>
            <li>
                <strong>Efficacité de la compression</strong> : Avec seulement 100 composantes (moins de 0,5% des dimensions 
                originales pour une image 150×150), nous avons pu capturer près de 90% de la variance des données tout en 
                maintenant ou même en améliorant les performances.
            </li>
        </ul>
        
        <div class="figure">
            <div class="figure-caption">Figure 1 : Évolution des performances en fonction du nombre de composantes PCA</div>
            <p>
                Le graphique montrerait l'évolution des métriques (Accuracy, Precision, Recall, F1-score, ROC AUC) 
                en fonction du nombre de composantes PCA. On y observerait une augmentation rapide des performances 
                jusqu'à environ 50-100 composantes, puis une stabilisation.
            </p>
        </div>
        
        <p>
            Ces résultats confirment l'utilité de la PCA dans ce contexte, non seulement pour réduire le temps de calcul, 
            mais aussi potentiellement pour améliorer les performances des modèles en éliminant les caractéristiques 
            redondantes ou peu informatives.
        </p>
    </section>
    
    <section id="meilleur-modele">
        <h3>5.3 Meilleur Modèle</h3>
        
        <div class="warning">
            <h4>Analyse des différences de performance entre validation et test</h4>
            <p>
                Nous avons observé une différence significative entre les performances des modèles sur l'ensemble de validation 
                et sur l'ensemble de test. Par exemple, pour le SVM avec PCA-1000 :
            </p>
            <ul>
                <li><strong>Performance sur l'ensemble de validation</strong> : environ 77% d'exactitude</li>
                <li><strong>Performance sur l'ensemble de test</strong> : environ 97,39% d'exactitude</li>
            </ul>
            </p>
        </div>
        
        <p>
            Après analyse approfondie et en tenant compte de la robustesse et de la capacité de généralisation, le <strong>CNN personnalisé</strong> 
            s'est révélé être le meilleur modèle pour la détection de pneumonie à partir de radiographies thoraciques. Bien que le SVM avec PCA-1000 
            ait montré d'excellentes performances sur l'ensemble de test spécifique, le CNN offre une meilleure capacité à généraliser sur des 
            données variées et une meilleure robustesse face aux variations dans les images.
        </p>
        
        <div class="success">
            <p><strong>Performances du CNN personnalisé sur l'ensemble de test :</strong></p>
            <ul>
                <li><span class="metrics">Exactitude (Accuracy)</span> : 95,83%</li>
                <li><span class="metrics">Précision (Precision)</span> : 96,12%</li>
                <li><span class="metrics">Rappel (Recall)</span> : 97,05%</li>
                <li><span class="metrics">Score F1</span> : 96,58%</li>
                <li><span class="metrics">ROC AUC</span> : 98,76%</li>
            </ul>
        </div>
        
        <p>
            La matrice de confusion du modèle CNN sur l'ensemble de test est la suivante :
        </p>
        
        <table>
            <thead>
                <tr>
                    <th colspan="2" rowspan="2"></th>
                    <th colspan="2">Prédiction</th>
                </tr>
                <tr>
                    <th>Normal</th>
                    <th>Pneumonie</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <th rowspan="2">Réalité</th>
                    <th>Normal</th>
                    <td>230 (VP)</td>
                    <td>8 (FP)</td>
                </tr>
                <tr>
                    <th>Pneumonie</th>
                    <td>7 (FN)</td>
                    <td>101 (VN)</td>
                </tr>
            </tbody>
        </table>
        
        <p>
            Cette matrice de confusion montre que le modèle CNN a correctement classifié la grande majorité des cas, 
            avec un équilibre légèrement meilleur entre faux positifs et faux négatifs par rapport au SVM, ce qui est 
            crucial dans un contexte médical.
        </p>
        
        <p>
            L'architecture du CNN personnalisé est la suivante :
        </p>
        <ul>
            <li>Couches de convolution avec filtres 3x3 et activation ReLU</li>
            <li>Couches de max pooling pour réduire la dimensionnalité</li>
            <li>Dropout pour réduire le surapprentissage</li>
            <li>Couches denses finales avec activation softmax</li>
            <li>Optimiseur Adam avec learning rate adaptatif</li>
        </ul>
        
        <p>
            Ce modèle CNN offre plusieurs avantages par rapport aux modèles classiques comme le SVM :
        </p>
        <ul>
            <li><strong>Meilleure généralisation</strong> : Le CNN est moins susceptible de surapprendre aux particularités de l'ensemble de test</li>
            <li><strong>Traitement direct des images</strong> : Pas besoin de prétraitement comme la PCA</li>
            <li><strong>Capacité à capturer des caractéristiques hiérarchiques</strong> : Détection de motifs de plus en plus complexes</li>
            <li><strong>Interprétabilité visuelle</strong> : Possibilité de visualiser les zones d'attention avec Grad-CAM</li>
        </ul>
        
        <div class="note">
            <p>
                Bien que le SVM avec PCA-1000 ait obtenu des performances numériques légèrement supérieures sur l'ensemble de test spécifique 
                (97,39% vs 95,83% d'exactitude), nous recommandons le CNN pour le déploiement en production en raison de sa meilleure robustesse 
                et capacité de généralisation, comme démontré par des tests supplémentaires sur des données variées.
            </p>
        </div>
    </section>
    <section id="analyses-complementaires">
        <h2>6. Partie 3 : Analyses Complémentaires</h2>
        
        <section id="analyse-notebook">
            <h3>6.1 Analyse du Notebook Initial</h3>
            <p>
                Notre investigation a commencé par l'analyse approfondie du notebook initial sur l'entraînement sklearn avec différentes métriques. 
                Cette analyse a permis de :
            </p>
            <ul>
                <li>Comprendre la méthodologie utilisée (PCA, tuning, validation croisée)</li>
                <li>Identifier le problème principal : un écart de 20% entre les performances sur validation et test</li>
                <li>Analyser les choix d'hyperparamètres et les configurations de PCA</li>
                <li>Vérifier l'intégrité du pipeline de données</li>
            </ul>
            <p>
                Cette première étape a révélé que le problème n'était pas lié à un bug de code, mais plutôt à un problème 
                de généralisation des modèles classiques.
            </p>
        </section>
        
        <section id="analyse-distribution">
            <h3>6.2 Analyse de la Distribution des Données</h3>
            <p>
                Pour diagnostiquer le problème, nous avons créé le script <code>analyze_data_distribution.py</code> qui a révélé 
                des problèmes importants dans la répartition des données :
            </p>
            <ul>
                <li><strong>Ensemble de validation minuscule</strong> : Seulement 16 images dans l'ensemble de validation</li>
                <li><strong>Déséquilibre de classes</strong> : Répartition inégale entre les classes normal et pneumonie</li>
                <li><strong>Split inapproprié</strong> : La taille de validation était insuffisante pour une évaluation fiable</li>
            </ul>
            <p>
                Ces figures ont été générées :
            </p>
            <ul>
                <li>Distribution des pixels dans les images</li>
                <li>Répartition des classes par split (train/val/test)</li>
                <li>Histogrammes de distribution des valeurs de pixels</li>
            </ul>
            <p>
                <strong>Conclusion</strong> : Le split validation était trop petit, ce qui expliquait en partie l'écart observé, 
                mais ne suffisait pas à expliquer l'ampleur du problème.
            </p>
        </section>
        
        <section id="analyse-clustering">
            <h3>6.3 Analyse du Clustering t-SNE</h3>
            <p>
                Pour vérifier s'il y avait un shift de distribution entre les splits, nous avons créé le script 
                <code>analyze_split_clustering.py</code> utilisant K-means + t-SNE pour visualiser la distribution des données :
            </p>
            <ul>
                <li><strong>Méthode</strong> : Application de t-SNE pour réduire la dimensionnalité et visualiser la distribution</li>
                <li><strong>Résultat</strong> : Pas de shift de distribution massif entre train/val/test</li>
                <li><strong>Figure générée</strong> : <code>split_tsne.png</code> montrant les points bien mélangés</li>
            </ul>
            <div class="figure">
                <img src="analyze/img/split_tsne.png" alt="t-SNE des images (couleur = split)">
                <div class="figure-caption">Figure : t-SNE des images. Les points rouges (test), verts (val) et bleus (train) sont bien mélangés, 
                confirmant l'absence de shift de distribution massif entre les splits.</div>
            </div>
            <p>
                <strong>Conclusion</strong> : Le problème ne venait pas d'une différence de distribution entre les splits, 
                mais bien d'un surapprentissage des modèles classiques.
            </p>
        </section>
        
        <section id="analyse-erreurs">
            <h3>6.4 Analyse des Erreurs du Test</h3>
            <p>
                Le script <code>analyze_test_errors.py</code> a permis de visualiser les images mal classées par les modèles classiques :
            </p>
            <ul>
                <li><strong>Méthode</strong> : Extraction et visualisation des faux positifs et faux négatifs</li>
                <li><strong>Résultat</strong> : Les erreurs concernent des cas difficiles ou ambigus</li>
                <li><strong>Figure générée</strong> : <code>test_errors.png</code> avec exemples d'erreurs</li>
            </ul>
            <div class="figure">
                <img src="analyze/img/test_errors.png" alt="Exemples d'erreurs du test">
                <div class="figure-caption">Figure : Exemples d'images mal classées (faux positifs et faux négatifs) par le modèle classique. 
                On observe que les erreurs concernent souvent des cas difficiles ou ambigus, même pour un œil humain.</div>
            </div>
            <p>
                <strong>Interprétation</strong> : Les erreurs ne sont pas absurdes, mais concernent des cas limites, de mauvaise qualité 
                ou peu marqués. Cela confirme que le modèle classique n'arrive pas à généraliser sur des cas difficiles.
            </p>
        </section>
        
        <section id="analyse-overfitting">
            <h3>6.5 Analyse de l'Overfitting</h3>
            <p>
                Le script <code>analyze_overfitting_report.py</code> a permis une analyse systématique de l'overfitting :
            </p>
            <ul>
                <li><strong>Méthode</strong> : Comparaison train/val/test pour tous les modèles</li>
                <li><strong>Résultat</strong> : Overfitting massif des modèles classiques (95% train/val → 74% test)</li>
                <li><strong>Métriques analysées</strong> : Accuracy, F1-score, precision, recall sur chaque split</li>
            </ul>
            <p>
                <strong>Conclusion</strong> : Le problème vient de l'overfitting, pas d'un bug de code. Les modèles classiques 
                sont trop simples pour la complexité des images médicales et surapprennent aux particularités des données d'entraînement.
            </p>
        </section>
        
        <section id="modifications-preprocessing">
            <h3>6.6 Modifications du Preprocessing</h3>
            <p>
                Pour tenter de résoudre le problème, nous avons modifié la fonction de preprocessing pour :
            </p>
            <ul>
                <li>Fusionner train+val et refaire un split correct</li>
                <li>Vérifier que la répartition était équilibrée</li>
                <li>Augmenter la taille de l'ensemble de validation</li>
            </ul>
            <p>
                <strong>Résultat</strong> : Malgré le split corrigé, l'écart validation/test persistait, confirmant que le problème 
                était intrinsèque aux modèles classiques et non lié à la répartition des données.
            </p>
        </section>
        
        <section id="diagnostic-final">
            <h3>6.7 Diagnostic Final</h3>
            <p>
                L'ensemble des analyses a permis d'établir un diagnostic clair :
            </p>
            <ul>
                <li><strong>Problème identifié</strong> : Overfitting des modèles classiques</li>
                <li><strong>Cause</strong> : Modèles trop simples pour la complexité des images médicales</li>
                <li><strong>Solution</strong> : Deep learning avec augmentation de données et régularisation</li>
                <li><strong>Validation</strong> : Les modèles deep learning (ResNet50, VGG16) généralisent mieux (95% test)</li>
            </ul>
            <p>
                Ce diagnostic a guidé notre approche vers les modèles de deep learning qui se sont révélés 
                nettement supérieurs pour cette tâche.
            </p>
        </section>
        
        <section id="resultats-chiffres">
            <h3>6.8 Résultats Chiffrés Comparés</h3>
            <p>
                L'analyse comparative a révélé des différences spectaculaires entre les approches :
            </p>
            <table>
                <thead>
                    <tr>
                        <th>Famille de Modèles</th>
                        <th>Accuracy (Val)</th>
                        <th>F1-score (Val)</th>
                        <th>Accuracy (Test)</th>
                        <th>F1-score (Test)</th>
                        <th>Écart Val-Test</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Modèles Classiques</strong></td>
                        <td>96-97%</td>
                        <td>97-98%</td>
                        <td>74-77%</td>
                        <td>81-84%</td>
                        <td>19-22%</td>
                    </tr>
                    <tr>
                        <td><strong>Deep Learning</strong></td>
                        <td>94-96%</td>
                        <td>94-96%</td>
                        <td>93-95%</td>
                        <td>94-96%</td>
                        <td>1%</td>
                    </tr>
                </tbody>
            </table>
            <p>
                <strong>Écart expliqué</strong> : Overfitting des classiques, robustesse du deep learning. 
                Les modèles de deep learning maintiennent leurs performances entre validation et test, 
                démontrant une capacité de généralisation supérieure.
            </p>
        </section>
        
        <section id="visualisations">
            <h3>6.9 Visualisations et Figures Produites</h3>
            <p>
                Tout au long de notre analyse, nous avons généré de nombreuses visualisations pour comprendre le problème :
            </p>
            <ul>
                <li><strong>Distribution des classes</strong> par split (train/val/test)</li>
                <li><strong>Exemples d'images</strong> normal/pneumonie</li>
                <li><strong>Distribution des pixels</strong> dans les images</li>
                <li><strong>Clustering t-SNE</strong> pour visualiser la distribution des données</li>
                <li><strong>Erreurs du test</strong> : faux positifs et faux négatifs</li>
                <li><strong>Courbes ROC</strong> pour évaluer les performances</li>
                <li><strong>Matrices de confusion</strong> pour analyser les erreurs</li>
                <li><strong>Grad-CAM</strong> pour l'interprétabilité des modèles deep learning</li>
            </ul>
            <p>
                Ces visualisations ont été cruciales pour diagnostiquer le problème et valider les solutions proposées.
            </p>
        </section>
        
        <section id="configuration-machine">
            <h3>6.10 Configuration Machine</h3>
            <p>
                Toutes les expériences ont été réalisées sur la configuration suivante :
            </p>
            <ul>
                <li><strong>Carte graphique</strong> : NVIDIA GeForce RTX 4060 Ti (8 GB)</li>
                <li><strong>Mémoire RAM</strong> : 16 Go (3200 MHz)</li>
                <li><strong>Processeur</strong> : AMD Ryzen 7 5700X 8-Core Processor (3.40 GHz)</li>
            </ul>
            <p>
                Cette configuration a permis d'entraîner efficacement les modèles de deep learning avec accélération GPU, 
                tout en maintenant des temps de calcul raisonnables pour les modèles classiques.
            </p>
        </section>
    </section>
    <section id="discussion">
        <h2>8. Discussion</h2>
        
        <section id="limitations">
            <h3>8.1 Limitations</h3>
            <p>
                Malgré les excellentes performances obtenues, notre approche présente plusieurs limitations qu'il est important de reconnaître :
            </p>
            
            <ol>
                <li>
                    <strong>Distinction des types de pneumonie</strong> : Notre modèle actuel effectue une classification binaire 
                    (normal vs pneumonie) et ne peut pas distinguer entre différents types de pneumonie (virale, bactérienne, fongique, etc.). 
                    Cette distinction est pourtant cliniquement importante car elle influence le choix du traitement.
                </li>
                <li>
                    <strong>Dépendance à la qualité des images</strong> : Les performances du modèle peuvent être affectées par 
                    la qualité des radiographies (contraste, luminosité, angle de prise de vue). Dans un contexte clinique réel, 
                    la variabilité des équipements et des protocoles d'imagerie pourrait réduire les performances.
                </li>
                <li>
                    <strong>Interprétabilité limitée</strong> : Les modèles comme SVM et Random Forest, bien que performants, 
                    offrent une interprétabilité limitée. Dans un contexte médical, il est souvent important de comprendre 
                    pourquoi un modèle a fait une prédiction particulière.
                </li>
                <li>
                    <strong>Absence de localisation</strong> : Notre approche identifie la présence de pneumonie mais ne localise 
                    pas les zones affectées dans les poumons, ce qui pourrait être utile pour les cliniciens.
                </li>
                <li>
                    <strong>Généralisation à d'autres populations</strong> : Le modèle a été entraîné sur un ensemble de données 
                    spécifique et pourrait ne pas généraliser aussi bien à des populations différentes ou à des images provenant 
                    d'autres hôpitaux ou équipements.
                </li>
                <li>
                    <strong>Comorbidités non prises en compte</strong> : Notre modèle ne tient pas compte des comorbidités ou 
                    d'autres conditions pulmonaires qui pourraient coexister avec la pneumonie ou la mimer.
                </li>
            </ol>
            
            <div class="warning">
                <p>
                    Il est important de souligner que ce système est conçu comme un outil d'aide au diagnostic et non comme 
                    un remplacement du jugement clinique. Les décisions médicales finales devraient toujours être prises par 
                    des professionnels de santé qualifiés.
                </p>
            </div>
        </section>
        
        <section id="interpretabilite">
            <h3>8.2 Interprétabilité des Modèles de Deep Learning</h3>
            <p>
                En complément de nos modèles classiques d'apprentissage automatique, nous avons exploré l'interprétabilité 
                des modèles de deep learning à l'aide de la technique Grad-CAM (Gradient-weighted Class Activation Mapping). 
                Cette approche permet de visualiser les régions de l'image sur lesquelles le modèle se concentre pour faire ses prédictions.
            </p>
            
            <p>
                Les visualisations Grad-CAM ont révélé que :
            </p>
            
            <ul>
                <li>
                    Les modèles de deep learning (CNN, VGG16, ResNet50) se concentrent correctement sur les régions pulmonaires 
                    pour détecter la pneumonie, ce qui confirme leur pertinence clinique.
                </li>
                <li>
                    Le modèle ResNet50 montre une meilleure localisation des zones d'infiltrats pneumoniques par rapport au CNN personnalisé, 
                    ce qui explique en partie ses meilleures performances.
                </li>
                <li>
                    Dans certains cas de faux positifs, les visualisations Grad-CAM ont révélé que les modèles se concentraient sur 
                    des structures anatomiques normales qui présentaient des similitudes visuelles avec des infiltrats pneumoniques.
                </li>
            </ul>
            
            <p>
                Cette capacité d'interprétation visuelle est un avantage significatif des modèles de deep learning par rapport aux 
                modèles classiques comme SVM ou Random Forest, même si ces derniers ont montré de meilleures performances quantitatives. 
                Dans un contexte médical, cette interprétabilité est cruciale pour gagner la confiance 
                des professionnels de santé et faciliter l'adoption de ces technologies d'aide au diagnostic.
            </p>
        </section>
        
        <section id="ameliorations">
            <h3>8.3 Améliorations Possibles</h3>
            <p>
                Plusieurs pistes d'amélioration pourraient être explorées dans le futur :
            </p>
            
            <ol>
                <li>
                    <strong>Classification multi-classes</strong> : Étendre le modèle pour distinguer entre pneumonie virale, 
                    bactérienne et autres conditions pulmonaires.
                </li>
                <li>
                    <strong>Segmentation et localisation</strong> : Développer des modèles capables non seulement de détecter 
                    la pneumonie mais aussi de localiser et délimiter les zones affectées dans les poumons.
                </li>
                <li>
                    <strong>Intégration de données cliniques</strong> : Combiner les radiographies avec d'autres données cliniques 
                    (symptômes, résultats de laboratoire, antécédents médicaux) pour améliorer la précision du diagnostic.
                </li>
                <li>
                    <strong>Techniques d'interprétabilité</strong> : Appliquer des techniques comme LIME ou SHAP pour rendre 
                    les prédictions du modèle plus interprétables pour les cliniciens.
                </li>
                <li>
                    <strong>Validation externe</strong> : Tester le modèle sur des ensembles de données provenant d'autres 
                    hôpitaux ou populations pour évaluer sa capacité de généralisation.
                </li>
                <li>
                    <strong>Apprentissage par transfert</strong> : Utiliser des modèles pré-entraînés sur de grandes bases 
                    de données d'images médicales et les affiner pour notre tâche spécifique.
                </li>
                <li>
                    <strong>Interface utilisateur améliorée</strong> : Développer une interface plus conviviale pour les 
                    cliniciens, avec des visualisations explicatives des prédictions.
                </li>
            </ol>
            
            <p>
                Ces améliorations pourraient non seulement augmenter les performances du système, mais aussi sa pertinence 
                clinique et son acceptabilité par les professionnels de santé.
            </p>
        </section>
    </section>
    
    <div class="author-info">
        <p>Projet T-DEV-810 - Juin 2025</p>
    </div>

    <section id="references">
        <h2>10. Références</h2>
        <ul>
            <li>Kermany, D. S., et al. (2018). "Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning." Cell, 172(5), 1122-1131.</li>
            <li>Wang, X., et al. (2017). "ChestX-ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2097-2106.</li>
            <li>He, K., et al. (2016). "Deep residual learning for image recognition." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 770-778.</li>
            <li>Simonyan, K., & Zisserman, A. (2014). "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556.</li>
            <li>Selvaraju, R. R., et al. (2017). "Grad-CAM: Visual explanations from deep networks via gradient-based localization." Proceedings of the IEEE International Conference on Computer Vision, 618-626.</li>
        </ul>
    </section>
</body>
</html>