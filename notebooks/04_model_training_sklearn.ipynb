{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ded1e229",
   "metadata": {},
   "source": [
    "# Entraînement de modèles pour la détection de pneumonie (scikit-learn avec et sans PCA)\n",
    "\n",
    "Ce notebook présente l'implémentation et l'entraînement de différents modèles pour la détection de pneumonie à partir de radiographies thoraciques, en utilisant scikit-learn avec et sans réduction de dimensionnalité PCA.\n",
    "\n",
    "Nous suivrons une approche progressive :\n",
    "1. Préparation des données avec la bonne répartition train/validation (75%/25%)\n",
    "2. Pour chaque modèle (Régression Logistique, Arbre de Décision, Random Forest):\n",
    "   - Entraînement sans PCA\n",
    "   - Entraînement avec PCA (10, 20, 50, 100, 1000 composantes)\n",
    "3. Pour SVM (en raison du temps de calcul):\n",
    "   - Entraînement uniquement avec PCA (10, 20, 50, 100, 1000 composantes)\n",
    "4. Comparaison des performances de toutes les configurations\n",
    "5. Sélection et évaluation du meilleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28422e61-5d4c-4eff-b23d-c1f39a1c1931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter le répertoire parent au chemin de recherche Python\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Obtenir le chemin absolu du répertoire parent\n",
    "notebook_dir = os.path.abspath('')\n",
    "project_dir = os.path.dirname(notebook_dir)\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63ba43ae-eced-4d62-92f5-7abb3d0e8075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports standard\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "# Imports scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Imports depuis notre projet\n",
    "from src.preprocessing import preprocess_images as preproc\n",
    "from src.visualization import visualizer as viz\n",
    "\n",
    "# Configuration pour afficher les images dans le notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 8)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b19245-9f78-4d50-a49e-87c607c99728",
   "metadata": {},
   "source": [
    "## 1. Préparation des données\n",
    "\n",
    "Nous allons utiliser notre module de prétraitement pour préparer les données avec une répartition train/validation de 75%/25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb82c68-f45e-4fee-b3e3-de19a31a693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les chemins vers les données\n",
    "data_dir = '../data/chest_Xray'\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "# Définir la taille des images\n",
    "img_size = (150, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "990a4f5e-27d9-4e93-88a9-465f8d01299a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données d'entraînement chargées: 5216 images\n",
      "  - Images normales: 1341\n",
      "  - Images avec pneumonie: 3875\n",
      "\n",
      "Répartition après division:\n",
      "Ensemble d'entraînement: 3912 images\n",
      "  - Images normales: 1006\n",
      "  - Images avec pneumonie: 2906\n",
      "Ensemble de validation: 1304 images\n",
      "  - Images normales: 335\n",
      "  - Images avec pneumonie: 969\n",
      "\n",
      "Ensemble de test: 624 images\n",
      "  - Images normales: 234\n",
      "  - Images avec pneumonie: 390\n"
     ]
    }
   ],
   "source": [
    "# Préparer les données avec notre module de prétraitement\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = preproc.preparer_donnees_pour_modele(\n",
    "    train_dir, test_dir, img_size=img_size, validation_size=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7870ff69-d033-43ba-80f3-4e1c2cf1671e",
   "metadata": {},
   "source": [
    "## 2. Redimensionner les données pour les modèles classiques\n",
    "\n",
    "Les modèles classiques de scikit-learn attendent des données sous forme de vecteurs 1D, pas des images 2D/3D. Nous devons donc redimensionner nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96a74db4-f16d-49d5-bdcf-2fa9e12ecebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme des données redimensionnées pour les modèles classiques:\n",
      "X_train: (3912, 22500)\n",
      "X_val: (1304, 22500)\n",
      "X_test: (624, 22500)\n"
     ]
    }
   ],
   "source": [
    "# Redimensionner les données pour les modèles classiques\n",
    "n_train = X_train.shape[0]\n",
    "n_val = X_val.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "# Calculer le nombre de caractéristiques\n",
    "n_features = np.prod(X_train.shape[1:])\n",
    "\n",
    "# Redimensionner les données\n",
    "X_train_reshaped = X_train.reshape(n_train, n_features)\n",
    "X_val_reshaped = X_val.reshape(n_val, n_features)\n",
    "X_test_reshaped = X_test.reshape(n_test, n_features)\n",
    "\n",
    "print(f\"Forme des données redimensionnées pour les modèles classiques:\")\n",
    "print(f\"X_train: {X_train_reshaped.shape}\")\n",
    "print(f\"X_val: {X_val_reshaped.shape}\")\n",
    "print(f\"X_test: {X_test_reshaped.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc9849a7-1d0f-496d-a456-bdb70d73d3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluer_modele(y_true, y_pred, y_proba=None):\n",
    "    \"\"\"\n",
    "    Évalue les performances d'un modèle et retourne les métriques.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Les vraies étiquettes\n",
    "        y_pred: Les prédictions du modèle\n",
    "        y_proba: Les probabilités prédites (pour ROC AUC)\n",
    "        \n",
    "    Returns:\n",
    "        Un dictionnaire contenant les métriques d'évaluation\n",
    "    \"\"\"\n",
    "    resultats = {}\n",
    "    \n",
    "    # Métriques de base\n",
    "    resultats['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    resultats['precision'] = precision_score(y_true, y_pred)\n",
    "    resultats['recall'] = recall_score(y_true, y_pred)\n",
    "    resultats['f1'] = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    resultats['confusion_matrix'] = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # ROC AUC si les probabilités sont fournies\n",
    "    if y_proba is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "        resultats['roc_auc'] = auc(fpr, tpr)\n",
    "        resultats['fpr'] = fpr\n",
    "        resultats['tpr'] = tpr\n",
    "    \n",
    "    return resultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79c8ee7-c4b9-45cd-a0dd-f8aa472a8644",
   "metadata": {},
   "source": [
    "## 3. Entraînement des modèles avec et sans PCA\n",
    "\n",
    "Nous allons maintenant entraîner chaque modèle dans différentes configurations:\n",
    "- Sans PCA (données originales) - sauf pour SVM\n",
    "- Avec PCA à 10 composantes\n",
    "- Avec PCA à 20 composantes\n",
    "- Avec PCA à 50 composantes\n",
    "- Avec PCA à 100 composantes\n",
    "- Avec PCA à 1000 composantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a322c81-9789-4948-a4ee-99f11a9ce91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des modèles à tester\n",
    "models = [\n",
    "    {\n",
    "        'name': 'Logistic Regression',\n",
    "        'model': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'param_grid': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'solver': ['liblinear'],\n",
    "            'penalty': ['l2']\n",
    "        },\n",
    "        'skip_no_pca': False  # Ne pas sauter l'entraînement sans PCA\n",
    "    },\n",
    "    {\n",
    "        'name': 'Decision Tree',\n",
    "        'model': DecisionTreeClassifier(random_state=42),\n",
    "        'param_grid': {\n",
    "            'max_depth': [None, 10],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'criterion': ['gini']\n",
    "        },\n",
    "        'skip_no_pca': False  # Ne pas sauter l'entraînement sans PCA\n",
    "    },\n",
    "    {\n",
    "        'name': 'Random Forest',\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [None, 10],\n",
    "            'min_samples_split': [2]\n",
    "        },\n",
    "        'skip_no_pca': False  # Ne pas sauter l'entraînement sans PCA\n",
    "    },\n",
    "    {\n",
    "        'name': 'SVM',\n",
    "        'model': SVC(probability=True, random_state=42),\n",
    "        'param_grid': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'gamma': ['scale'],\n",
    "            'kernel': ['rbf']\n",
    "        },\n",
    "        'skip_no_pca': True  # Sauter l'entraînement sans PCA pour SVM\n",
    "    }\n",
    "]\n",
    "\n",
    "# Configurations PCA à tester\n",
    "pca_components = [10, 20, 50, 100, 1000]\n",
    "\n",
    "# Dictionnaire pour stocker tous les résultats\n",
    "all_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202c2930-3c43-490d-b004-c1dafe2ebd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Modèle: Logistic Regression\n",
      "======================================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Configuration: Sans PCA\n",
      "--------------------------------------------------\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    }
   ],
   "source": [
    "# Dictionnaire pour stocker tous les résultats\n",
    "all_results = {}\n",
    "\n",
    "# Entraînement de tous les modèles\n",
    "for model_info in models:\n",
    "    model_name = model_info['name']\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Modèle: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Dictionnaire pour stocker les résultats de ce modèle\n",
    "    model_results = {}\n",
    "    \n",
    "    # Entraînement sans PCA (sauf pour SVM)\n",
    "    if not model_info['skip_no_pca']:\n",
    "        print(f\"\\n{'-'*50}\")\n",
    "        print(f\"Configuration: Sans PCA\")\n",
    "        print(f\"{'-'*50}\")\n",
    "        \n",
    "        # Créer une instance de GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model_info['model'],\n",
    "            param_grid=model_info['param_grid'],\n",
    "            cv=3,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Mesurer le temps d'entraînement\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Entraîner le modèle\n",
    "        grid_search.fit(X_train_reshaped, y_train)\n",
    "        \n",
    "        # Calculer le temps d'entraînement\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Meilleurs paramètres et score\n",
    "        print(f\"\\nMeilleurs paramètres: {grid_search.best_params_}\")\n",
    "        print(f\"Meilleur score de validation croisée: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"Temps d'entraînement: {training_time:.2f} secondes\")\n",
    "        \n",
    "        # Prédictions sur l'ensemble de validation\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_val_pred = best_model.predict(X_val_reshaped)\n",
    "        y_val_proba = best_model.predict_proba(X_val_reshaped)[:, 1]\n",
    "        \n",
    "        # Évaluation du modèle\n",
    "        resultats = evaluer_modele(y_val, y_val_pred, y_val_proba)\n",
    "        \n",
    "        # Afficher les résultats\n",
    "        print(\"\\nPerformances sur l'ensemble de validation:\")\n",
    "        print(f\"Accuracy: {resultats['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {resultats['precision']:.4f}\")\n",
    "        print(f\"Recall: {resultats['recall']:.4f}\")\n",
    "        print(f\"F1-score: {resultats['f1']:.4f}\")\n",
    "        print(f\"ROC AUC: {resultats['roc_auc']:.4f}\")\n",
    "        \n",
    "        # Stocker les résultats\n",
    "        model_results['Sans PCA'] = {\n",
    "            'best_model': best_model,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'training_time': training_time,\n",
    "            'resultats': resultats\n",
    "        }\n",
    "    \n",
    "    # Entraînement avec différentes configurations PCA\n",
    "    for n_comp in pca_components:\n",
    "        # Vérifier si le nombre de composantes est valide\n",
    "        if n_comp > min(X_train_reshaped.shape[0], X_train_reshaped.shape[1]):\n",
    "            print(f\"\\nSkipping PCA with {n_comp} components (too many for the dataset)\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n{'-'*50}\")\n",
    "        print(f\"Configuration: PCA avec {n_comp} composantes\")\n",
    "        print(f\"{'-'*50}\")\n",
    "        \n",
    "        # Appliquer PCA\n",
    "        pca = PCA(n_components=n_comp, random_state=42)\n",
    "        X_train_pca = pca.fit_transform(X_train_reshaped)\n",
    "        X_val_pca = pca.transform(X_val_reshaped)\n",
    "        \n",
    "        # Variance expliquée\n",
    "        variance_explained = np.sum(pca.explained_variance_ratio_)\n",
    "        print(f\"Variance expliquée: {variance_explained:.4f}\")\n",
    "        \n",
    "        # Créer une instance de GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model_info['model'],\n",
    "            param_grid=model_info['param_grid'],\n",
    "            cv=3,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Mesurer le temps d'entraînement\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Entraîner le modèle\n",
    "        grid_search.fit(X_train_pca, y_train)\n",
    "        \n",
    "        # Calculer le temps d'entraînement\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Meilleurs paramètres et score\n",
    "        print(f\"\\nMeilleurs paramètres: {grid_search.best_params_}\")\n",
    "        print(f\"Meilleur score de validation croisée: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"Temps d'entraînement: {training_time:.2f} secondes\")\n",
    "        \n",
    "        # Prédictions sur l'ensemble de validation\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_val_pred = best_model.predict(X_val_pca)\n",
    "        y_val_proba = best_model.predict_proba(X_val_pca)[:, 1]\n",
    "        \n",
    "        # Évaluation du modèle\n",
    "        resultats = evaluer_modele(y_val, y_val_pred, y_val_proba)\n",
    "        \n",
    "        # Afficher les résultats\n",
    "        print(\"\\nPerformances sur l'ensemble de validation:\")\n",
    "        print(f\"Accuracy: {resultats['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {resultats['precision']:.4f}\")\n",
    "        print(f\"Recall: {resultats['recall']:.4f}\")\n",
    "        print(f\"F1-score: {resultats['f1']:.4f}\")\n",
    "        print(f\"ROC AUC: {resultats['roc_auc']:.4f}\")\n",
    "        \n",
    "        # Stocker les résultats\n",
    "        model_results[f'PCA-{n_comp}'] = {\n",
    "            'best_model': best_model,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'pca': pca,\n",
    "            'training_time': training_time,\n",
    "            'resultats': resultats,\n",
    "            'variance_explained': variance_explained\n",
    "        }\n",
    "    \n",
    "    # Stocker les résultats de ce modèle\n",
    "    all_results[model_name] = model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ccd40f-40aa-44cc-810c-022364bb783a",
   "metadata": {},
   "source": [
    "## 4. Comparaison des résultats\n",
    "\n",
    "Nous allons maintenant comparer les performances des différents modèles avec et sans PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800def43-1d85-476f-a006-0b40ac42c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour chaque modèle, créer un tableau comparatif des différentes configurations\n",
    "for model_name, model_configs in all_results.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Comparaison des configurations pour {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Préparation des données pour le tableau comparatif\n",
    "    configs = []\n",
    "    metriques = ['Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC', 'Temps (s)']\n",
    "    valeurs = []\n",
    "    \n",
    "    for config_name, config_data in model_configs.items():\n",
    "        configs.append(config_name)\n",
    "        \n",
    "        res = config_data['resultats']\n",
    "        valeurs.append([\n",
    "            res['accuracy'],\n",
    "            res['precision'],\n",
    "            res['recall'],\n",
    "            res['f1'],\n",
    "            res['roc_auc'],\n",
    "            config_data['training_time']\n",
    "        ])\n",
    "    \n",
    "    # Affichage du tableau comparatif\n",
    "    viz.afficher_tableau_comparatif_modeles(configs, metriques, valeurs, \n",
    "                                           titre=f\"Comparaison des configurations - {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a51ec7-6ae5-4c0e-98f1-41d12b259553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données pour le tableau comparatif global\n",
    "# Nous allons comparer la meilleure configuration de chaque modèle\n",
    "modeles = []\n",
    "metriques = ['Meilleure config', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC', 'Temps (s)']\n",
    "valeurs = []\n",
    "\n",
    "for model_name, model_configs in all_results.items():\n",
    "    modeles.append(model_name)\n",
    "    \n",
    "    # Trouver la meilleure configuration (basée sur F1-score)\n",
    "    best_config = None\n",
    "    best_f1 = 0\n",
    "    \n",
    "    for config_name, config_data in model_configs.items():\n",
    "        if config_data['resultats']['f1'] > best_f1:\n",
    "            best_f1 = config_data['resultats']['f1']\n",
    "            best_config = config_name\n",
    "    \n",
    "    # Récupérer les données de la meilleure configuration\n",
    "    best_data = model_configs[best_config]\n",
    "    res = best_data['resultats']\n",
    "    \n",
    "    valeurs.append([\n",
    "        best_config,\n",
    "        res['accuracy'],\n",
    "        res['precision'],\n",
    "        res['recall'],\n",
    "        res['f1'],\n",
    "        res['roc_auc'],\n",
    "        best_data['training_time']\n",
    "    ])\n",
    "\n",
    "# Affichage du tableau comparatif global\n",
    "viz.afficher_tableau_comparatif_modeles(modeles, metriques, valeurs, \n",
    "                                       titre=\"Comparaison des meilleurs modèles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd00a26-eec3-424a-95e1-0a432ecf8933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'évolution des performances en fonction du nombre de composantes PCA\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Métriques à visualiser\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    \n",
    "    for model_name, model_configs in all_results.items():\n",
    "        # Extraire les configurations PCA\n",
    "        pca_configs = {k: v for k, v in model_configs.items() if k.startswith('PCA-')}\n",
    "        \n",
    "        if pca_configs:\n",
    "            # Trier par nombre de composantes\n",
    "            components = [int(k.split('-')[1]) for k in pca_configs.keys()]\n",
    "            values = [v['resultats'][metric] for v in pca_configs.values()]\n",
    "            \n",
    "            # Tracer la courbe\n",
    "            plt.plot(components, values, marker='o', label=model_name)\n",
    "    \n",
    "    plt.title(f'{metric_names[i]} vs Nombre de composantes PCA')\n",
    "    plt.xlabel('Nombre de composantes PCA')\n",
    "    plt.ylabel(metric_names[i])\n",
    "    plt.xscale('log')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e747366f-329d-4af0-8b21-717b9ab774ab",
   "metadata": {},
   "source": [
    "## 5. Sélection et évaluation du meilleur modèle\n",
    "\n",
    "Maintenant que nous avons comparé les différentes configurations, nous allons sélectionner le meilleur modèle global et l'évaluer sur l'ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c593c6-dc5d-4ee8-8971-a9342dee6e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver le meilleur modèle global (basé sur F1-score)\n",
    "best_model_name = None\n",
    "best_config_name = None\n",
    "best_f1 = 0\n",
    "\n",
    "for model_name, model_configs in all_results.items():\n",
    "    for config_name, config_data in model_configs.items():\n",
    "        if config_data['resultats']['f1'] > best_f1:\n",
    "            best_f1 = config_data['resultats']['f1']\n",
    "            best_model_name = model_name\n",
    "            best_config_name = config_name\n",
    "\n",
    "print(f\"Le meilleur modèle est: {best_model_name} avec la configuration {best_config_name}\")\n",
    "print(f\"F1-score: {best_f1:.4f}\")\n",
    "\n",
    "# Récupérer les données du meilleur modèle\n",
    "best_model_data = all_results[best_model_name][best_config_name]\n",
    "best_model = best_model_data['best_model']\n",
    "\n",
    "# Préparation des données de test\n",
    "if best_config_name.startswith('PCA'):\n",
    "    # Récupérer le transformateur PCA\n",
    "    pca = best_model_data['pca']\n",
    "    X_test_transformed = pca.transform(X_test_reshaped)\n",
    "else:\n",
    "    X_test_transformed = X_test_reshaped\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_test_pred = best_model.predict(X_test_transformed)\n",
    "y_test_proba = best_model.predict_proba(X_test_transformed)[:, 1]\n",
    "\n",
    "# Évaluation des performances\n",
    "test_results = evaluer_modele(y_test, y_test_pred, y_test_proba)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\nPerformances du meilleur modèle sur l'ensemble de test:\")\n",
    "print(f\"Accuracy: {test_results['accuracy']:.4f}\")\n",
    "print(f\"Precision: {test_results['precision']:.4f}\")\n",
    "print(f\"Recall: {test_results['recall']:.4f}\")\n",
    "print(f\"F1-score: {test_results['f1']:.4f}\")\n",
    "print(f\"ROC AUC: {test_results['roc_auc']:.4f}\")\n",
    "\n",
    "# Affichage de la matrice de confusion\n",
    "cm = test_results['confusion_matrix']\n",
    "print(\"\\nMatrice de confusion:\")\n",
    "print(cm)\n",
    "\n",
    "# Affichage du rapport de classification\n",
    "print(\"\\nRapport de classification:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Normal', 'Pneumonie']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87a29e9-9ce2-4bb2-8df2-572b71e2811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la matrice de confusion\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=cm, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Normal', 'Pneumonie'],\n",
    "            yticklabels=['Normal', 'Pneumonie'])\n",
    "plt.title(f'Matrice de confusion - {best_model_name} ({best_config_name}) - Test')\n",
    "plt.xlabel('Prédiction')\n",
    "plt.ylabel('Réalité')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualisation de la courbe ROC\n",
    "plt.figure(figsize=(8, 6))\n",
    "fpr = test_results['fpr']\n",
    "tpr = test_results['tpr']\n",
    "roc_auc = test_results['roc_auc']\n",
    "\n",
    "plt.plot(fpr, tpr, lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title(f'Courbe ROC - {best_model_name} ({best_config_name}) - Test')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9316e3de-9f0e-48ab-af99-b759212e6b1a",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "Dans ce notebook, nous avons entraîné et évalué plusieurs modèles de classification pour la détection de pneumonie à partir de radiographies thoraciques, avec et sans réduction de dimensionnalité PCA.\n",
    "\n",
    "Les principales observations sont:\n",
    "1. Nous avons testé chaque modèle (sauf SVM) sans PCA et avec PCA à différents nombres de composantes (10, 20, 50, 100, 1000).\n",
    "2. Pour SVM, en raison du temps de calcul, nous l'avons uniquement testé avec PCA.\n",
    "3. Nous avons comparé les performances des différentes configurations et identifié la meilleure.\n",
    "4. Le meilleur modèle a été évalué sur l'ensemble de test et a montré de bonnes performances.\n",
    "\n",
    "Cette analyse nous permet de comprendre l'impact de la réduction de dimensionnalité PCA sur les performances des modèles et de choisir la configuration optimale pour notre tâche de détection de pneumonie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55bba4b-d5b2-4fcb-b3f7-7fa6b6d8f1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du meilleur modèle\n",
    "model_path = f'../models/best_model_{best_model_name.replace(\" \", \"_\").lower()}_{best_config_name.replace(\"-\", \"_\").lower()}.pkl'\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "joblib.dump(best_model, model_path)\n",
    "\n",
    "# Sauvegarde du transformateur PCA si nécessaire\n",
    "if best_config_name.startswith('PCA'):\n",
    "    pca_path = f'../models/pca_transformer_{best_config_name.replace(\"-\", \"_\").lower()}.pkl'\n",
    "    joblib.dump(pca, pca_path)\n",
    "    print(f\"Transformateur PCA sauvegardé: {pca_path}\")\n",
    "\n",
    "print(f\"Meilleur modèle sauvegardé: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ddfb24-5357-4493-8747-a1dddf10a1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
