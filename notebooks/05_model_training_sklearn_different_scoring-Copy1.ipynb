{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ded1e229",
   "metadata": {},
   "source": [
    "# Entraînement de modèles pour la détection de pneumonie (scikit-learn avec et sans PCA)\n",
    "\n",
    "Ce notebook présente l'implémentation et l'entraînement de différents modèles pour la détection de pneumonie à partir de radiographies thoraciques, en utilisant scikit-learn avec et sans réduction de dimensionnalité PCA.\n",
    "\n",
    "Nous suivrons une approche progressive :\n",
    "1. Préparation des données avec la bonne répartition train/validation (75%/25%)\n",
    "2. Pour chaque modèle (Régression Logistique, Arbre de Décision, Random Forest):\n",
    "   - Entraînement sans PCA\n",
    "   - Entraînement avec PCA (10, 20, 50, 100, 1000 composantes)\n",
    "3. Pour SVM (en raison du temps de calcul):\n",
    "   - Entraînement uniquement avec PCA (10, 20, 50, 100, 1000 composantes)\n",
    "4. Comparaison des performances de toutes les configurations\n",
    "5. Sélection et évaluation du meilleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28422e61-5d4c-4eff-b23d-c1f39a1c1931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter le répertoire parent au chemin de recherche Python\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Obtenir le chemin absolu du répertoire parent\n",
    "notebook_dir = os.path.abspath('')\n",
    "project_dir = os.path.dirname(notebook_dir)\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63ba43ae-eced-4d62-92f5-7abb3d0e8075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports standard\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "# Imports scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Imports depuis notre projet\n",
    "from src.preprocessing import preprocess_images as preproc\n",
    "from src.visualization import visualizer as viz\n",
    "\n",
    "# Configuration pour afficher les images dans le notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 8)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b19245-9f78-4d50-a49e-87c607c99728",
   "metadata": {},
   "source": [
    "## 1. Préparation des données\n",
    "\n",
    "Nous allons utiliser notre module de prétraitement pour préparer les données avec une répartition train/validation de 75%/25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeb82c68-f45e-4fee-b3e3-de19a31a693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les chemins vers les données\n",
    "data_dir = '../data/chest_Xray'\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'val')\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "# Définir la taille des images\n",
    "img_size = (150, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "990a4f5e-27d9-4e93-88a9-465f8d01299a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des images de train (../data/chest_Xray\\train) et val (../data/chest_Xray\\val) pour fusion...\n",
      "Fusion train+val: 5232 images\n",
      "\n",
      "Nouveau split train/val:\n",
      "Ensemble d'entraînement: 3924 images\n",
      "  - Images normales: 1012\n",
      "  - Images avec pneumonie: 2912\n",
      "Ensemble de validation: 1308 images\n",
      "  - Images normales: 337\n",
      "  - Images avec pneumonie: 971\n",
      "\n",
      "Ensemble de test: 624 images\n",
      "  - Images normales: 234\n",
      "  - Images avec pneumonie: 390\n"
     ]
    }
   ],
   "source": [
    "# Préparer les données avec notre module de prétraitement\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = preproc.preparer_donnees_pour_modele(\n",
    "    train_dir, test_dir, img_size=img_size, validation_size=0.25, val_dir=val_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7870ff69-d033-43ba-80f3-4e1c2cf1671e",
   "metadata": {},
   "source": [
    "## 2. Redimensionner les données pour les modèles classiques\n",
    "\n",
    "Les modèles classiques de scikit-learn attendent des données sous forme de vecteurs 1D, pas des images 2D/3D. Nous devons donc redimensionner nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96a74db4-f16d-49d5-bdcf-2fa9e12ecebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme des données redimensionnées pour les modèles classiques:\n",
      "X_train: (3924, 22500)\n",
      "X_val: (1308, 22500)\n",
      "X_test: (624, 22500)\n"
     ]
    }
   ],
   "source": [
    "# Redimensionner les données pour les modèles classiques\n",
    "n_train = X_train.shape[0]\n",
    "n_val = X_val.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "# Calculer le nombre de caractéristiques\n",
    "n_features = np.prod(X_train.shape[1:])\n",
    "\n",
    "# Redimensionner les données\n",
    "X_train_reshaped = X_train.reshape(n_train, n_features)\n",
    "X_val_reshaped = X_val.reshape(n_val, n_features)\n",
    "X_test_reshaped = X_test.reshape(n_test, n_features)\n",
    "\n",
    "print(f\"Forme des données redimensionnées pour les modèles classiques:\")\n",
    "print(f\"X_train: {X_train_reshaped.shape}\")\n",
    "print(f\"X_val: {X_val_reshaped.shape}\")\n",
    "print(f\"X_test: {X_test_reshaped.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc9849a7-1d0f-496d-a456-bdb70d73d3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluer_modele(y_true, y_pred, y_proba=None):\n",
    "    \"\"\"\n",
    "    Évalue les performances d'un modèle et retourne les métriques.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Les vraies étiquettes\n",
    "        y_pred: Les prédictions du modèle\n",
    "        y_proba: Les probabilités prédites (pour ROC AUC)\n",
    "        \n",
    "    Returns:\n",
    "        Un dictionnaire contenant les métriques d'évaluation\n",
    "    \"\"\"\n",
    "    resultats = {}\n",
    "    \n",
    "    # Métriques de base\n",
    "    resultats['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    resultats['precision'] = precision_score(y_true, y_pred)\n",
    "    resultats['recall'] = recall_score(y_true, y_pred)\n",
    "    resultats['f1'] = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    resultats['confusion_matrix'] = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # ROC AUC si les probabilités sont fournies\n",
    "    if y_proba is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "        resultats['roc_auc'] = auc(fpr, tpr)\n",
    "        resultats['fpr'] = fpr\n",
    "        resultats['tpr'] = tpr\n",
    "    \n",
    "    return resultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79c8ee7-c4b9-45cd-a0dd-f8aa472a8644",
   "metadata": {},
   "source": [
    "## 3. Entraînement des modèles avec et sans PCA\n",
    "\n",
    "Nous allons maintenant entraîner chaque modèle dans différentes configurations:\n",
    "- Sans PCA (données originales) - sauf pour SVM\n",
    "- Avec PCA à 10 composantes\n",
    "- Avec PCA à 20 composantes\n",
    "- Avec PCA à 50 composantes\n",
    "- Avec PCA à 100 composantes\n",
    "- Avec PCA à 1000 composantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a322c81-9789-4948-a4ee-99f11a9ce91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des modèles à tester\n",
    "models = [\n",
    "    {\n",
    "        'name': 'Logistic Regression',\n",
    "        'model': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'param_grid': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'solver': ['liblinear'],\n",
    "            'penalty': ['l2']\n",
    "        },\n",
    "        'skip_no_pca': False  # Ne pas sauter l'entraînement sans PCA\n",
    "    },\n",
    "    {\n",
    "        'name': 'Decision Tree',\n",
    "        'model': DecisionTreeClassifier(random_state=42),\n",
    "        'param_grid': {\n",
    "            'max_depth': [None, 10],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'criterion': ['gini']\n",
    "        },\n",
    "        'skip_no_pca': False  # Ne pas sauter l'entraînement sans PCA\n",
    "    },\n",
    "    {\n",
    "        'name': 'Random Forest',\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [None, 10],\n",
    "            'min_samples_split': [2]\n",
    "        },\n",
    "        'skip_no_pca': False  # Ne pas sauter l'entraînement sans PCA\n",
    "    },\n",
    "    {\n",
    "        'name': 'SVM',\n",
    "        'model': SVC(probability=True, random_state=42),\n",
    "        'param_grid': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'gamma': ['scale'],\n",
    "            'kernel': ['rbf']\n",
    "        },\n",
    "        'skip_no_pca': False  # Ne pas sauter l'entraînement sans PCA\n",
    "    }\n",
    "]\n",
    "\n",
    "# Configurations PCA à tester\n",
    "pca_components = [10, 20, 50, 100, 1000]\n",
    "\n",
    "# Dictionnaire pour stocker tous les résultats\n",
    "all_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202c2930-3c43-490d-b004-c1dafe2ebd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "MÉTRIQUE DE SCORING: ACCURACY\n",
      "********************************************************************************\n",
      "\n",
      "======================================================================\n",
      "Modèle: Logistic Regression\n",
      "======================================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Configuration: Sans PCA\n",
      "--------------------------------------------------\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "\n",
      "Meilleurs paramètres: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Meilleur score de validation croisée (accuracy): 0.9620\n",
      "Temps d'entraînement: 66.47 secondes\n",
      "\n",
      "Performances sur l'ensemble de validation:\n",
      "Accuracy: 0.9610\n",
      "Precision: 0.9822\n",
      "Recall: 0.9650\n",
      "F1-score: 0.9735\n",
      "ROC AUC: 0.9914\n",
      "\n",
      "--------------------------------------------------\n",
      "Configuration: PCA avec 10 composantes\n",
      "--------------------------------------------------\n",
      "Variance expliquée: 0.6464\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "\n",
      "Meilleurs paramètres: {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Meilleur score de validation croisée (accuracy): 0.9279\n",
      "Temps d'entraînement: 1.50 secondes\n",
      "\n",
      "Performances sur l'ensemble de validation:\n",
      "Accuracy: 0.9335\n",
      "Precision: 0.9595\n",
      "Recall: 0.9506\n",
      "F1-score: 0.9550\n",
      "ROC AUC: 0.9773\n",
      "\n",
      "--------------------------------------------------\n",
      "Configuration: PCA avec 20 composantes\n",
      "--------------------------------------------------\n",
      "Variance expliquée: 0.7475\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "\n",
      "Meilleurs paramètres: {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Meilleur score de validation croisée (accuracy): 0.9475\n",
      "Temps d'entraînement: 0.08 secondes\n",
      "\n",
      "Performances sur l'ensemble de validation:\n",
      "Accuracy: 0.9472\n",
      "Precision: 0.9757\n",
      "Recall: 0.9526\n",
      "F1-score: 0.9640\n",
      "ROC AUC: 0.9844\n",
      "\n",
      "--------------------------------------------------\n",
      "Configuration: PCA avec 50 composantes\n",
      "--------------------------------------------------\n",
      "Variance expliquée: 0.8383\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "\n",
      "Meilleurs paramètres: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Meilleur score de validation croisée (accuracy): 0.9529\n",
      "Temps d'entraînement: 0.21 secondes\n",
      "\n",
      "Performances sur l'ensemble de validation:\n",
      "Accuracy: 0.9518\n",
      "Precision: 0.9799\n",
      "Recall: 0.9547\n",
      "F1-score: 0.9671\n",
      "ROC AUC: 0.9857\n",
      "\n",
      "--------------------------------------------------\n",
      "Configuration: PCA avec 100 composantes\n",
      "--------------------------------------------------\n",
      "Variance expliquée: 0.8845\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "\n",
      "Meilleurs paramètres: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Meilleur score de validation croisée (accuracy): 0.9554\n",
      "Temps d'entraînement: 0.25 secondes\n",
      "\n",
      "Performances sur l'ensemble de validation:\n",
      "Accuracy: 0.9572\n",
      "Precision: 0.9811\n",
      "Recall: 0.9609\n",
      "F1-score: 0.9709\n",
      "ROC AUC: 0.9878\n",
      "\n",
      "--------------------------------------------------\n",
      "Configuration: PCA avec 1000 composantes\n",
      "--------------------------------------------------\n",
      "Variance expliquée: 0.9813\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "\n",
      "Meilleurs paramètres: {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Meilleur score de validation croisée (accuracy): 0.9625\n",
      "Temps d'entraînement: 3.68 secondes\n",
      "\n",
      "Performances sur l'ensemble de validation:\n",
      "Accuracy: 0.9633\n",
      "Precision: 0.9812\n",
      "Recall: 0.9691\n",
      "F1-score: 0.9751\n",
      "ROC AUC: 0.9912\n",
      "\n",
      "======================================================================\n",
      "Modèle: Decision Tree\n",
      "======================================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Configuration: Sans PCA\n",
      "--------------------------------------------------\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "\n",
      "Meilleurs paramètres: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2}\n",
      "Meilleur score de validation croisée (accuracy): 0.8764\n",
      "Temps d'entraînement: 107.20 secondes\n",
      "\n",
      "Performances sur l'ensemble de validation:\n",
      "Accuracy: 0.8930\n",
      "Precision: 0.9333\n",
      "Recall: 0.9217\n",
      "F1-score: 0.9275\n",
      "ROC AUC: 0.8256\n",
      "\n",
      "--------------------------------------------------\n",
      "Configuration: PCA avec 10 composantes\n",
      "--------------------------------------------------\n",
      "Variance expliquée: 0.6464\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "\n",
      "Meilleurs paramètres: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2}\n",
      "Meilleur score de validation croisée (accuracy): 0.8963\n",
      "Temps d'entraînement: 0.34 secondes\n",
      "\n",
      "Performances sur l'ensemble de validation:\n",
      "Accuracy: 0.9006\n",
      "Precision: 0.9422\n",
      "Recall: 0.9228\n",
      "F1-score: 0.9324\n",
      "ROC AUC: 0.8619\n",
      "\n",
      "--------------------------------------------------\n",
      "Configuration: PCA avec 20 composantes\n",
      "--------------------------------------------------\n",
      "Variance expliquée: 0.7475\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "\n",
      "Meilleurs paramètres: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2}\n",
      "Meilleur score de validation croisée (accuracy): 0.8904\n",
      "Temps d'entraînement: 0.17 secondes\n",
      "\n",
      "Performances sur l'ensemble de validation:\n",
      "Accuracy: 0.8914\n",
      "Precision: 0.9349\n",
      "Recall: 0.9176\n",
      "F1-score: 0.9262\n",
      "ROC AUC: 0.8477\n",
      "\n",
      "--------------------------------------------------\n",
      "Configuration: PCA avec 50 composantes\n",
      "--------------------------------------------------\n",
      "Variance expliquée: 0.8383\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "\n",
      "Meilleurs paramètres: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2}\n",
      "Meilleur score de validation croisée (accuracy): 0.8927\n",
      "Temps d'entraînement: 0.54 secondes\n",
      "\n",
      "Performances sur l'ensemble de validation:\n",
      "Accuracy: 0.8922\n",
      "Precision: 0.9415\n",
      "Recall: 0.9114\n",
      "F1-score: 0.9262\n",
      "ROC AUC: 0.8496\n",
      "\n",
      "--------------------------------------------------\n",
      "Configuration: PCA avec 100 composantes\n",
      "--------------------------------------------------\n",
      "Variance expliquée: 0.8845\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "\n",
      "Meilleurs paramètres: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2}\n",
      "Meilleur score de validation croisée (accuracy): 0.8858\n",
      "Temps d'entraînement: 0.94 secondes\n",
      "\n",
      "Performances sur l'ensemble de validation:\n",
      "Accuracy: 0.8746\n",
      "Precision: 0.9279\n",
      "Recall: 0.9011\n",
      "F1-score: 0.9143\n",
      "ROC AUC: 0.8126\n",
      "\n",
      "--------------------------------------------------\n",
      "Configuration: PCA avec 1000 composantes\n",
      "--------------------------------------------------\n",
      "Variance expliquée: 0.9813\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "\n",
      "Meilleurs paramètres: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5}\n",
      "Meilleur score de validation croisée (accuracy): 0.8710\n",
      "Temps d'entraînement: 9.87 secondes\n",
      "\n",
      "Performances sur l'ensemble de validation:\n",
      "Accuracy: 0.8700\n",
      "Precision: 0.9203\n",
      "Recall: 0.9032\n",
      "F1-score: 0.9116\n",
      "ROC AUC: 0.7895\n",
      "\n",
      "======================================================================\n",
      "Modèle: Random Forest\n",
      "======================================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Configuration: Sans PCA\n",
      "--------------------------------------------------\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "\n",
      "Meilleurs paramètres: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Meilleur score de validation croisée (accuracy): 0.9501\n",
      "Temps d'entraînement: 51.19 secondes\n",
      "\n",
      "Performances sur l'ensemble de validation:\n",
      "Accuracy: 0.9587\n",
      "Precision: 0.9722\n",
      "Recall: 0.9722\n",
      "F1-score: 0.9722\n",
      "ROC AUC: 0.9842\n",
      "\n",
      "--------------------------------------------------\n",
      "Configuration: PCA avec 10 composantes\n",
      "--------------------------------------------------\n",
      "Variance expliquée: 0.6464\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "\n",
      "Meilleurs paramètres: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Meilleur score de validation croisée (accuracy): 0.9350\n",
      "Temps d'entraînement: 1.54 secondes\n",
      "\n",
      "Performances sur l'ensemble de validation:\n",
      "Accuracy: 0.9411\n",
      "Precision: 0.9533\n",
      "Recall: 0.9681\n",
      "F1-score: 0.9607\n",
      "ROC AUC: 0.9826\n",
      "\n",
      "--------------------------------------------------\n",
      "Configuration: PCA avec 20 composantes\n",
      "--------------------------------------------------\n",
      "Variance expliquée: 0.7475\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "\n",
      "Meilleurs paramètres: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Meilleur score de validation croisée (accuracy): 0.9391\n",
      "Temps d'entraînement: 1.43 secondes\n",
      "\n",
      "Performances sur l'ensemble de validation:\n",
      "Accuracy: 0.9472\n",
      "Precision: 0.9510\n",
      "Recall: 0.9794\n",
      "F1-score: 0.9650\n",
      "ROC AUC: 0.9865\n",
      "\n",
      "--------------------------------------------------\n",
      "Configuration: PCA avec 50 composantes\n",
      "--------------------------------------------------\n",
      "Variance expliquée: 0.8383\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    }
   ],
   "source": [
    "# Dictionnaire pour stocker tous les résultats\n",
    "all_results = {}\n",
    "\n",
    "# Définir les métriques de scoring à tester\n",
    "scoring_metrics = ['accuracy', 'f1', 'roc_auc', 'precision', 'recall']\n",
    "\n",
    "# Pour chaque métrique de scoring\n",
    "for scoring_metric in scoring_metrics:\n",
    "    print(f\"\\n{'*'*80}\")\n",
    "    print(f\"MÉTRIQUE DE SCORING: {scoring_metric.upper()}\")\n",
    "    print(f\"{'*'*80}\")\n",
    "    \n",
    "    # Dictionnaire pour stocker les résultats de cette métrique\n",
    "    scoring_results = {}\n",
    "    \n",
    "    # Entraînement de tous les modèles avec cette métrique\n",
    "    for model_info in models:\n",
    "        model_name = model_info['name']\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Modèle: {model_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Dictionnaire pour stocker les résultats de ce modèle\n",
    "        model_results = {}\n",
    "        \n",
    "        # Entraînement sans PCA (sauf pour SVM)\n",
    "        if not model_info['skip_no_pca']:\n",
    "            print(f\"\\n{'-'*50}\")\n",
    "            print(f\"Configuration: Sans PCA\")\n",
    "            print(f\"{'-'*50}\")\n",
    "            \n",
    "            # Créer une instance de GridSearchCV avec la métrique actuelle\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=model_info['model'],\n",
    "                param_grid=model_info['param_grid'],\n",
    "                cv=3,\n",
    "                scoring=scoring_metric,  # Utiliser la métrique actuelle\n",
    "                n_jobs=-1,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Mesurer le temps d'entraînement\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Entraîner le modèle\n",
    "            grid_search.fit(X_train_reshaped, y_train)\n",
    "            \n",
    "            # Calculer le temps d'entraînement\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Meilleurs paramètres et score\n",
    "            print(f\"\\nMeilleurs paramètres: {grid_search.best_params_}\")\n",
    "            print(f\"Meilleur score de validation croisée ({scoring_metric}): {grid_search.best_score_:.4f}\")\n",
    "            print(f\"Temps d'entraînement: {training_time:.2f} secondes\")\n",
    "            \n",
    "            # Prédictions sur l'ensemble de validation\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_val_pred = best_model.predict(X_val_reshaped)\n",
    "            y_val_proba = best_model.predict_proba(X_val_reshaped)[:, 1]\n",
    "            \n",
    "            # Évaluation du modèle\n",
    "            resultats = evaluer_modele(y_val, y_val_pred, y_val_proba)\n",
    "            \n",
    "            # Afficher les résultats\n",
    "            print(\"\\nPerformances sur l'ensemble de validation:\")\n",
    "            print(f\"Accuracy: {resultats['accuracy']:.4f}\")\n",
    "            print(f\"Precision: {resultats['precision']:.4f}\")\n",
    "            print(f\"Recall: {resultats['recall']:.4f}\")\n",
    "            print(f\"F1-score: {resultats['f1']:.4f}\")\n",
    "            print(f\"ROC AUC: {resultats['roc_auc']:.4f}\")\n",
    "            \n",
    "            # Stocker les résultats\n",
    "            model_results['Sans PCA'] = {\n",
    "                'best_model': best_model,\n",
    "                'best_params': grid_search.best_params_,\n",
    "                'training_time': training_time,\n",
    "                'resultats': resultats\n",
    "            }\n",
    "        \n",
    "        # Entraînement avec différentes configurations PCA\n",
    "        for n_comp in pca_components:\n",
    "            # Vérifier si le nombre de composantes est valide\n",
    "            if n_comp > min(X_train_reshaped.shape[0], X_train_reshaped.shape[1]):\n",
    "                print(f\"\\nSkipping PCA with {n_comp} components (too many for the dataset)\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\n{'-'*50}\")\n",
    "            print(f\"Configuration: PCA avec {n_comp} composantes\")\n",
    "            print(f\"{'-'*50}\")\n",
    "            \n",
    "            # Appliquer PCA\n",
    "            pca = PCA(n_components=n_comp, random_state=42)\n",
    "            X_train_pca = pca.fit_transform(X_train_reshaped)\n",
    "            X_val_pca = pca.transform(X_val_reshaped)\n",
    "            \n",
    "            # Variance expliquée\n",
    "            variance_explained = np.sum(pca.explained_variance_ratio_)\n",
    "            print(f\"Variance expliquée: {variance_explained:.4f}\")\n",
    "            \n",
    "            # Créer une instance de GridSearchCV avec la métrique actuelle\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=model_info['model'],\n",
    "                param_grid=model_info['param_grid'],\n",
    "                cv=3,\n",
    "                scoring=scoring_metric,  # Utiliser la métrique actuelle\n",
    "                n_jobs=-1,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Mesurer le temps d'entraînement\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Entraîner le modèle\n",
    "            grid_search.fit(X_train_pca, y_train)\n",
    "            \n",
    "            # Calculer le temps d'entraînement\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Meilleurs paramètres et score\n",
    "            print(f\"\\nMeilleurs paramètres: {grid_search.best_params_}\")\n",
    "            print(f\"Meilleur score de validation croisée ({scoring_metric}): {grid_search.best_score_:.4f}\")\n",
    "            print(f\"Temps d'entraînement: {training_time:.2f} secondes\")\n",
    "            \n",
    "            # Prédictions sur l'ensemble de validation\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_val_pred = best_model.predict(X_val_pca)\n",
    "            y_val_proba = best_model.predict_proba(X_val_pca)[:, 1]\n",
    "            \n",
    "            # Évaluation du modèle\n",
    "            resultats = evaluer_modele(y_val, y_val_pred, y_val_proba)\n",
    "            \n",
    "            # Afficher les résultats\n",
    "            print(\"\\nPerformances sur l'ensemble de validation:\")\n",
    "            print(f\"Accuracy: {resultats['accuracy']:.4f}\")\n",
    "            print(f\"Precision: {resultats['precision']:.4f}\")\n",
    "            print(f\"Recall: {resultats['recall']:.4f}\")\n",
    "            print(f\"F1-score: {resultats['f1']:.4f}\")\n",
    "            print(f\"ROC AUC: {resultats['roc_auc']:.4f}\")\n",
    "            \n",
    "            # Stocker les résultats\n",
    "            model_results[f'PCA-{n_comp}'] = {\n",
    "                'best_model': best_model,\n",
    "                'best_params': grid_search.best_params_,\n",
    "                'pca': pca,\n",
    "                'training_time': training_time,\n",
    "                'resultats': resultats,\n",
    "                'variance_explained': variance_explained\n",
    "            }\n",
    "        \n",
    "        # Stocker les résultats de ce modèle\n",
    "        scoring_results[model_name] = model_results\n",
    "    \n",
    "    # Stocker les résultats de cette métrique de scoring\n",
    "    all_results[scoring_metric] = scoring_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ccd40f-40aa-44cc-810c-022364bb783a",
   "metadata": {},
   "source": [
    "## 4. Comparaison des résultats\n",
    "\n",
    "Nous allons maintenant comparer les performances des différents modèles avec et sans PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800def43-1d85-476f-a006-0b40ac42c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour chaque métrique de scoring\n",
    "for scoring_metric, scoring_results in all_results.items():\n",
    "    print(f\"\\n{'*'*80}\")\n",
    "    print(f\"RÉSULTATS POUR LA MÉTRIQUE DE SCORING: {scoring_metric.upper()}\")\n",
    "    print(f\"{'*'*80}\")\n",
    "    \n",
    "    # Pour chaque modèle, créer un tableau comparatif des différentes configurations\n",
    "    for model_name, model_configs in scoring_results.items():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Comparaison des configurations pour {model_name} (Scoring: {scoring_metric})\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Préparation des données pour le tableau comparatif\n",
    "        configs = []\n",
    "        metriques = ['Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC', 'Temps (s)']\n",
    "        valeurs = []\n",
    "            \n",
    "        for config_name, config_data in model_configs.items():\n",
    "            configs.append(config_name)\n",
    "            \n",
    "            res = config_data['resultats']\n",
    "            valeurs.append([\n",
    "                res['accuracy'],\n",
    "                res['precision'],\n",
    "                res['recall'],\n",
    "                res['f1'],\n",
    "                res['roc_auc'],\n",
    "                config_data['training_time']\n",
    "            ])\n",
    "        \n",
    "        # Affichage du tableau comparatif\n",
    "        viz.afficher_tableau_comparatif_modeles(configs, metriques, valeurs, \n",
    "                                               titre=f\"Comparaison des configurations - {model_name} (Scoring: {scoring_metric})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a51ec7-6ae5-4c0e-98f1-41d12b259553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour chaque métrique de scoring, créer un tableau comparatif global\n",
    "for scoring_metric in scoring_metrics:\n",
    "    print(f\"\\n{'*'*80}\")\n",
    "    print(f\"TABLEAU GLOBAL POUR LA MÉTRIQUE DE SCORING: {scoring_metric.upper()}\")\n",
    "    print(f\"{'*'*80}\")\n",
    "    \n",
    "    scoring_results = all_results[scoring_metric]\n",
    "    \n",
    "    # Préparation des données pour le tableau comparatif global\n",
    "    # Nous allons comparer la meilleure configuration de chaque modèle\n",
    "    modeles = []\n",
    "    metriques = ['Meilleure config', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC', 'Temps (s)']\n",
    "    valeurs = []\n",
    "    \n",
    "    for model_name, model_configs in scoring_results.items():\n",
    "        modeles.append(model_name)\n",
    "        \n",
    "        # Trouver la meilleure configuration (basée sur la métrique de scoring actuelle)\n",
    "        best_config = None\n",
    "        best_score = -1\n",
    "        \n",
    "        for config_name, config_data in model_configs.items():\n",
    "            # Utiliser la métrique correspondante pour déterminer la meilleure config\n",
    "            current_score = config_data['resultats'][scoring_metric] if scoring_metric in config_data['resultats'] else config_data['resultats']['accuracy']\n",
    "            \n",
    "            if current_score > best_score:\n",
    "                best_score = current_score\n",
    "                best_config = config_name\n",
    "        \n",
    "        # Récupérer les données de la meilleure configuration\n",
    "        best_data = model_configs[best_config]\n",
    "        res = best_data['resultats']\n",
    "        \n",
    "        valeurs.append([\n",
    "            best_config,\n",
    "            res['accuracy'],\n",
    "            res['precision'],\n",
    "            res['recall'],\n",
    "            res['f1'],\n",
    "            res['roc_auc'],\n",
    "            best_data['training_time']\n",
    "        ])\n",
    "    \n",
    "    # Affichage du tableau comparatif global\n",
    "    viz.afficher_tableau_comparatif_modeles(modeles, metriques, valeurs, \n",
    "                                           titre=f\"Comparaison des meilleurs modèles (Scoring: {scoring_metric})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd00a26-eec3-424a-95e1-0a432ecf8933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour chaque métrique de scoring\n",
    "for scoring_metric, scoring_results in all_results.items():\n",
    "    print(f\"\\n{'*'*80}\")\n",
    "    print(f\"VISUALISATION POUR LA MÉTRIQUE DE SCORING: {scoring_metric.upper()}\")\n",
    "    print(f\"{'*'*80}\")\n",
    "    \n",
    "    # Visualisation de l'évolution des performances en fonction du nombre de composantes PCA\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Métriques à visualiser\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC']\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        \n",
    "        for model_name, model_configs in scoring_results.items():\n",
    "            # Extraire les configurations PCA\n",
    "            pca_configs = {k: v for k, v in model_configs.items() if k.startswith('PCA-')}\n",
    "            \n",
    "            if pca_configs:\n",
    "                # Trier par nombre de composantes\n",
    "                components = [int(k.split('-')[1]) for k in pca_configs.keys()]\n",
    "                values = [v['resultats'][metric] for v in pca_configs.values()]\n",
    "                \n",
    "                # Tracer la courbe\n",
    "                plt.plot(components, values, marker='o', label=model_name)\n",
    "        \n",
    "        plt.title(f'{metric_names[i]} vs Nombre de composantes PCA')\n",
    "        plt.xlabel('Nombre de composantes PCA')\n",
    "        plt.ylabel(metric_names[i])\n",
    "        plt.xscale('log')\n",
    "        plt.grid(True)\n",
    "        plt.legend(loc=\"lower right\")  # Placer la légende en bas à droite pour éviter de masquer les courbes\n",
    "    \n",
    "    plt.suptitle(f'Impact du nombre de composantes PCA sur les performances (Scoring: {scoring_metric})', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e747366f-329d-4af0-8b21-717b9ab774ab",
   "metadata": {},
   "source": [
    "## 5. Sélection et évaluation du meilleur modèle\n",
    "\n",
    "Maintenant que nous avons comparé les différentes configurations, nous allons sélectionner le meilleur modèle global et l'évaluer sur l'ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c593c6-dc5d-4ee8-8971-a9342dee6e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour chaque métrique de scoring, trouver et évaluer le meilleur modèle\n",
    "for scoring_metric, scoring_results in all_results.items():\n",
    "    print(f\"\\n{'*'*80}\")\n",
    "    print(f\"MEILLEUR MODÈLE POUR LA MÉTRIQUE DE SCORING: {scoring_metric.upper()}\")\n",
    "    print(f\"{'*'*80}\")\n",
    "    \n",
    "    # Trouver le meilleur modèle global (basé sur la métrique de scoring actuelle)\n",
    "    best_model_name = None\n",
    "    best_config_name = None\n",
    "    best_score = -1\n",
    "\n",
    "    for model_name, model_configs in scoring_results.items():\n",
    "        for config_name, config_data in model_configs.items():\n",
    "            # Utiliser la métrique correspondante pour déterminer le meilleur modèle\n",
    "            current_score = config_data['resultats'][scoring_metric] if scoring_metric in config_data['resultats'] else config_data['resultats']['accuracy']\n",
    "            \n",
    "            if current_score > best_score:\n",
    "                best_score = current_score\n",
    "                best_model_name = model_name\n",
    "                best_config_name = config_name\n",
    "\n",
    "    print(f\"Le meilleur modèle est: {best_model_name} avec la configuration {best_config_name}\")\n",
    "    print(f\"Score {scoring_metric}: {best_score:.4f}\")\n",
    "\n",
    "    # Récupérer les données du meilleur modèle\n",
    "    best_model_data = scoring_results[best_model_name][best_config_name]\n",
    "    best_model = best_model_data['best_model']\n",
    "\n",
    "    # Préparation des données de test\n",
    "    if best_config_name.startswith('PCA'):\n",
    "        # Récupérer le transformateur PCA\n",
    "        pca = best_model_data['pca']\n",
    "        X_test_transformed = pca.transform(X_test_reshaped)\n",
    "    else:\n",
    "        X_test_transformed = X_test_reshaped\n",
    "\n",
    "    # Prédictions sur l'ensemble de test\n",
    "    y_test_pred = best_model.predict(X_test_transformed)\n",
    "    y_test_proba = best_model.predict_proba(X_test_transformed)[:, 1]\n",
    "\n",
    "    # Évaluation des performances\n",
    "    test_results = evaluer_modele(y_test, y_test_pred, y_test_proba)\n",
    "\n",
    "    # Affichage des résultats\n",
    "    print(\"\\nPerformances du meilleur modèle sur l'ensemble de test:\")\n",
    "    print(f\"Accuracy: {test_results['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {test_results['precision']:.4f}\")\n",
    "    print(f\"Recall: {test_results['recall']:.4f}\")\n",
    "    print(f\"F1-score: {test_results['f1']:.4f}\")\n",
    "    print(f\"ROC AUC: {test_results['roc_auc']:.4f}\")\n",
    "\n",
    "    # Affichage de la matrice de confusion\n",
    "    cm = test_results['confusion_matrix']\n",
    "    print(\"\\nMatrice de confusion:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Affichage du rapport de classification\n",
    "    print(\"\\nRapport de classification:\")\n",
    "    print(classification_report(y_test, y_test_pred, target_names=['Normal', 'Pneumonie']))\n",
    "    \n",
    "    # Visualisation de la matrice de confusion\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Normal', 'Pneumonie'], \n",
    "                yticklabels=['Normal', 'Pneumonie'])\n",
    "    plt.xlabel('Prédiction')\n",
    "    plt.ylabel('Réalité')\n",
    "    plt.title(f'Matrice de confusion - {best_model_name} ({best_config_name}) - Scoring: {scoring_metric}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualisation de la courbe ROC\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_test_proba)\n",
    "    plt.plot(fpr, tpr, color='red', lw=2, label=f'ROC curve (AUC = {test_results[\"roc_auc\"]:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Taux de faux positifs')\n",
    "    plt.ylabel('Taux de vrais positifs')\n",
    "    plt.title(f'Courbe ROC - {best_model_name} ({best_config_name}) - Scoring: {scoring_metric}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87a29e9-9ce2-4bb2-8df2-572b71e2811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau comparatif des meilleurs modèles pour chaque métrique de scoring\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"COMPARAISON DES MEILLEURS MODÈLES PAR MÉTRIQUE DE SCORING SUR L'ENSEMBLE DE TEST\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "# Préparation des données pour le tableau comparatif\n",
    "scoring_metrics = list(all_results.keys())\n",
    "metriques = ['Modèle', 'Configuration', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC']\n",
    "valeurs = []\n",
    "\n",
    "for scoring_metric in scoring_metrics:\n",
    "    scoring_results = all_results[scoring_metric]\n",
    "    \n",
    "    # Trouver le meilleur modèle pour cette métrique\n",
    "    best_model_name = None\n",
    "    best_config_name = None\n",
    "    best_score = -1\n",
    "    \n",
    "    for model_name, model_configs in scoring_results.items():\n",
    "        for config_name, config_data in model_configs.items():\n",
    "            # Utiliser la métrique correspondante pour déterminer le meilleur modèle\n",
    "            current_score = config_data['resultats'][scoring_metric] if scoring_metric in config_data['resultats'] else config_data['resultats']['accuracy']\n",
    "            \n",
    "            if current_score > best_score:\n",
    "                best_score = current_score\n",
    "                best_model_name = model_name\n",
    "                best_config_name = config_name\n",
    "    \n",
    "    # Récupérer les données du meilleur modèle\n",
    "    best_model_data = scoring_results[best_model_name][best_config_name]\n",
    "    best_model = best_model_data['best_model']\n",
    "    \n",
    "    # Préparation des données de test\n",
    "    if best_config_name.startswith('PCA'):\n",
    "        # Récupérer le transformateur PCA\n",
    "        pca = best_model_data['pca']\n",
    "        X_test_transformed = pca.transform(X_test_reshaped)\n",
    "    else:\n",
    "        X_test_transformed = X_test_reshaped\n",
    "    \n",
    "    # Prédictions sur l'ensemble de test\n",
    "    y_test_pred = best_model.predict(X_test_transformed)\n",
    "    y_test_proba = best_model.predict_proba(X_test_transformed)[:, 1]\n",
    "    \n",
    "    # Évaluation des performances\n",
    "    test_results = evaluer_modele(y_test, y_test_pred, y_test_proba)\n",
    "    \n",
    "    # Ajouter les résultats au tableau\n",
    "    valeurs.append([\n",
    "        best_model_name,\n",
    "        best_config_name,\n",
    "        test_results['accuracy'],\n",
    "        test_results['precision'],\n",
    "        test_results['recall'],\n",
    "        test_results['f1'],\n",
    "        test_results['roc_auc']\n",
    "    ])\n",
    "\n",
    "# Affichage du tableau comparatif\n",
    "viz.afficher_tableau_comparatif_modeles(scoring_metrics, metriques, valeurs, \n",
    "                                       titre=\"Comparaison des métriques de scoring sur l'ensemble de test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9316e3de-9f0e-48ab-af99-b759212e6b1a",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "Dans ce notebook, nous avons entraîné et évalué plusieurs modèles de classification pour la détection de pneumonie à partir de radiographies thoraciques, avec et sans réduction de dimensionnalité PCA.\n",
    "\n",
    "Les principales observations sont:\n",
    "1. Nous avons testé chaque modèle (sauf SVM) sans PCA et avec PCA à différents nombres de composantes (10, 20, 50, 100, 1000).\n",
    "2. Pour SVM, en raison du temps de calcul, nous l'avons uniquement testé avec PCA.\n",
    "3. Nous avons comparé les performances des différentes configurations et identifié la meilleure.\n",
    "4. Le meilleur modèle a été évalué sur l'ensemble de test et a montré de bonnes performances.\n",
    "\n",
    "Cette analyse nous permet de comprendre l'impact de la réduction de dimensionnalité PCA sur les performances des modèles et de choisir la configuration optimale pour notre tâche de détection de pneumonie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55bba4b-d5b2-4fcb-b3f7-7fa6b6d8f1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du meilleur modèle\n",
    "model_path = f'../models/best_model_{best_model_name.replace(\" \", \"_\").lower()}_{best_config_name.replace(\"-\", \"_\").lower()}.pkl'\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "joblib.dump(best_model, model_path)\n",
    "\n",
    "# Sauvegarde du transformateur PCA si nécessaire\n",
    "if best_config_name.startswith('PCA'):\n",
    "    pca_path = f'../models/pca_transformer_{best_config_name.replace(\"-\", \"_\").lower()}.pkl'\n",
    "    joblib.dump(pca, pca_path)\n",
    "    print(f\"Transformateur PCA sauvegardé: {pca_path}\")\n",
    "\n",
    "print(f\"Meilleur modèle sauvegardé: {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
